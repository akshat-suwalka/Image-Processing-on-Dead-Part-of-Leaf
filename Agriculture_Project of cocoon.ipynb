{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Agriculture Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gNgHRB61RiO",
        "colab_type": "text"
      },
      "source": [
        "# For inception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO4pgY8CelTU",
        "colab_type": "code",
        "outputId": "4aa7e95e-69a2-4fa4-8875-a275a6d94835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (300, 300, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-07 10:46:51--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 2404:6800:4008:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "\r          /tmp/ince   0%[                    ]       0  --.-KB/s               \r         /tmp/incep   4%[                    ]   4.01M  14.4MB/s               \r        /tmp/incept   9%[>                   ]   8.01M  16.4MB/s               \r       /tmp/incepti  47%[========>           ]  40.01M  40.1MB/s               \r      /tmp/inceptio  85%[================>   ]  72.01M  59.3MB/s               \r     /tmp/inception  95%[==================> ]  80.01M  55.2MB/s               \r/tmp/inception_v3_w 100%[===================>]  83.84M  57.3MB/s    in 1.5s    \n",
            "\n",
            "2019-10-07 10:46:53 (57.3 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n",
            "last layer output shape:  (None, 17, 17, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuGRUCkLs7Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "139749e7-0414-4599-fc9b-4c5fd88b8efe",
        "id": "qmvW6MpzZgyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "494c260c-8ef1-4e1c-d01a-cba2a265e6dc",
        "id": "UOxST-cFZqtr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/Cocoon Dataset.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/content/Cocoon Dataset'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'Train' )\n",
        "validation_dir = os.path.join( base_dir, 'Test')\n",
        "\n",
        "\n",
        "train_Male_dir = os.path.join(train_dir, 'Male') # Directory with our training cat pictures\n",
        "train_Female_dir = os.path.join(train_dir, 'Female') # Directory with our training dog pictures\n",
        "validation_Male_dir = os.path.join(validation_dir, 'Male') # Directory with our validation cat pictures\n",
        "validation_Female_dir = os.path.join(validation_dir, 'Female')# Directory with our validation dog pictures\n",
        "\n",
        "train_Male_fnames = os.listdir(train_Male_dir)\n",
        "train_Female_fnames = os.listdir(train_Female_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 21,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (300, 300))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 10,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (300, 300))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 357 images belonging to 2 classes.\n",
            "Found 90 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Tph4ycZQ3m",
        "colab_type": "code",
        "outputId": "f2f4423a-08a9-4cd6-cd52-3b7bb20b3221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch = train_generator.samples/train_generator.batch_size ,\n",
        "            epochs = 30,\n",
        "            validation_data = validation_generator,\n",
        "            validation_steps = validation_generator.samples/validation_generator.batch_size,\n",
        "            verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "17/17 [==============================] - 131s 8s/step - loss: 1.8335 - acc: 0.4986 - val_loss: 0.7370 - val_acc: 0.5556\n",
            "Epoch 2/30\n",
            "17/17 [==============================] - 106s 6s/step - loss: 0.7555 - acc: 0.5294 - val_loss: 0.7421 - val_acc: 0.5444\n",
            "Epoch 3/30\n",
            "17/17 [==============================] - 106s 6s/step - loss: 0.7500 - acc: 0.5630 - val_loss: 0.7144 - val_acc: 0.4222\n",
            "Epoch 4/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.7195 - acc: 0.5686 - val_loss: 1.1237 - val_acc: 0.5556\n",
            "Epoch 5/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.7762 - acc: 0.5546 - val_loss: 0.8968 - val_acc: 0.5556\n",
            "Epoch 6/30\n",
            "17/17 [==============================] - 106s 6s/step - loss: 0.7240 - acc: 0.5714 - val_loss: 0.8078 - val_acc: 0.4111\n",
            "Epoch 7/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.7116 - acc: 0.5882 - val_loss: 0.8170 - val_acc: 0.4111\n",
            "Epoch 8/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.7364 - acc: 0.5434 - val_loss: 0.6914 - val_acc: 0.5222\n",
            "Epoch 9/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.6801 - acc: 0.5602 - val_loss: 0.8335 - val_acc: 0.4444\n",
            "Epoch 10/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.7045 - acc: 0.5910 - val_loss: 0.7359 - val_acc: 0.4667\n",
            "Epoch 11/30\n",
            "17/17 [==============================] - 104s 6s/step - loss: 0.6971 - acc: 0.6134 - val_loss: 0.7886 - val_acc: 0.4444\n",
            "Epoch 12/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6693 - acc: 0.5938 - val_loss: 0.7780 - val_acc: 0.4333\n",
            "Epoch 13/30\n",
            "17/17 [==============================] - 103s 6s/step - loss: 0.6941 - acc: 0.5714 - val_loss: 0.7888 - val_acc: 0.5556\n",
            "Epoch 14/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.6841 - acc: 0.5966 - val_loss: 0.7235 - val_acc: 0.3556\n",
            "Epoch 15/30\n",
            "17/17 [==============================] - 104s 6s/step - loss: 0.6842 - acc: 0.5854 - val_loss: 0.7698 - val_acc: 0.5556\n",
            "Epoch 16/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.6717 - acc: 0.5938 - val_loss: 0.8019 - val_acc: 0.5556\n",
            "Epoch 17/30\n",
            "17/17 [==============================] - 105s 6s/step - loss: 0.6721 - acc: 0.6022 - val_loss: 0.7807 - val_acc: 0.5556\n",
            "Epoch 18/30\n",
            "17/17 [==============================] - 103s 6s/step - loss: 0.6728 - acc: 0.5854 - val_loss: 0.9714 - val_acc: 0.5556\n",
            "Epoch 19/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6835 - acc: 0.6106 - val_loss: 0.7553 - val_acc: 0.3111\n",
            "Epoch 20/30\n",
            "16/17 [===========================>..] - ETA: 3s - loss: 0.6784 - acc: 0.6042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a3e16a76b8c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             verbose = 1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-oE65P--kAA",
        "colab_type": "code",
        "outputId": "145dc4bf-d933-4adf-8de4-cf50e9c88804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 300, 300, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 149, 149, 32) 864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 149, 149, 32) 96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 149, 149, 32) 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 147, 147, 32) 9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 147, 147, 32) 96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 147, 147, 32) 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 147, 147, 64) 18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 147, 147, 64) 192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 147, 147, 64) 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 73, 73, 80)   240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 73, 73, 80)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 71, 71, 192)  138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 71, 71, 192)  576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 71, 71, 192)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 35, 35, 64)   192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 35, 35, 64)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 35, 35, 96)   55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 35, 35, 48)   144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 35, 35, 96)   288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 35, 35, 48)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 35, 35, 96)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 35, 35, 64)   76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 35, 35, 96)   82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 35, 35, 64)   192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 35, 35, 64)   192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 35, 35, 96)   288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 35, 35, 32)   96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 35, 35, 64)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 35, 35, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 35, 35, 96)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 35, 35, 32)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 35, 35, 64)   192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 35, 35, 64)   0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 35, 35, 96)   55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 35, 35, 48)   144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 35, 35, 96)   288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 35, 35, 48)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 35, 35, 96)   0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 35, 35, 64)   76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 35, 35, 96)   82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 35, 35, 64)   192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 35, 35, 64)   192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 35, 35, 96)   288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 35, 35, 64)   192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 35, 35, 64)   0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 35, 35, 64)   0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 35, 35, 96)   0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 35, 35, 64)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 35, 35, 64)   192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 35, 35, 64)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 35, 35, 96)   55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 35, 35, 48)   144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 35, 35, 96)   288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 35, 35, 48)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 35, 35, 96)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 35, 35, 64)   76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 35, 35, 96)   82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 35, 35, 64)   192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 35, 35, 64)   192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 35, 35, 96)   288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 35, 35, 64)   192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 35, 35, 64)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 35, 35, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 35, 35, 96)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 35, 35, 64)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 35, 35, 64)   192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 35, 35, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 35, 35, 96)   55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 35, 35, 96)   288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 35, 35, 96)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 17, 17, 96)   82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 17, 17, 384)  1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 17, 17, 96)   288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 17, 17, 384)  0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 17, 17, 96)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 17, 17, 128)  384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 17, 17, 128)  0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 17, 17, 128)  114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 17, 17, 128)  384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 17, 17, 128)  0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 17, 17, 128)  114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 17, 17, 128)  384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 17, 17, 128)  384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 17, 17, 128)  0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 17, 17, 128)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 17, 17, 128)  114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 17, 17, 128)  114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 17, 17, 128)  384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 17, 17, 128)  384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 17, 17, 128)  0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 17, 17, 128)  0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 17, 17, 192)  172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 17, 17, 192)  172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 17, 17, 192)  576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 17, 17, 192)  576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 17, 17, 192)  576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 17, 17, 192)  576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 17, 17, 192)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 17, 17, 192)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 17, 17, 192)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 17, 17, 192)  0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 17, 17, 160)  480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 17, 17, 160)  0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 17, 17, 160)  179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 17, 17, 160)  480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 17, 17, 160)  0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 17, 17, 160)  179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 17, 17, 160)  480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 17, 17, 160)  480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 17, 17, 160)  0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 17, 17, 160)  0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 17, 17, 160)  179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 17, 17, 160)  179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 17, 17, 160)  480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 17, 17, 160)  480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 17, 17, 160)  0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 17, 17, 160)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 17, 17, 192)  215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 17, 17, 192)  215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 17, 17, 192)  576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 17, 17, 192)  576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 17, 17, 192)  576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 17, 17, 192)  576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 17, 17, 192)  0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 17, 17, 192)  0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 17, 17, 192)  0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 17, 17, 192)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 17, 17, 160)  480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 17, 17, 160)  0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 17, 17, 160)  179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 17, 17, 160)  480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 17, 17, 160)  0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 17, 17, 160)  179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 17, 17, 160)  480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 17, 17, 160)  480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 17, 17, 160)  0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 17, 17, 160)  0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 17, 17, 160)  179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 17, 17, 160)  179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 17, 17, 160)  480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 17, 17, 160)  480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 17, 17, 160)  0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 17, 17, 160)  0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 17, 17, 192)  215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 17, 17, 192)  215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 17, 17, 192)  576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 17, 17, 192)  576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 17, 17, 192)  576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 17, 17, 192)  576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 17, 17, 192)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 17, 17, 192)  0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 17, 17, 192)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 17, 17, 192)  0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 17, 17, 192)  576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 17, 17, 192)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 17, 17, 192)  258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 17, 17, 192)  576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 17, 17, 192)  0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 17, 17, 192)  258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 17, 17, 192)  576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 17, 17, 192)  576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 17, 17, 192)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 17, 17, 192)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 17, 17, 192)  258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 17, 17, 192)  258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 17, 17, 192)  576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 17, 17, 192)  576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 17, 17, 192)  0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 17, 17, 192)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 17, 17, 192)  258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 17, 17, 192)  258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 17, 17, 192)  576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 17, 17, 192)  576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 17, 17, 192)  576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 17, 17, 192)  576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 17, 17, 192)  0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 17, 17, 192)  0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 17, 17, 192)  0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 17, 17, 192)  0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 221952)       0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1024)         227279872   flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            1025        dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 236,256,161\n",
            "Trainable params: 227,280,897\n",
            "Non-trainable params: 8,975,264\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRfQZvsPCemd",
        "colab_type": "code",
        "outputId": "f26c586b-2472-43a4-a22e-d0b474ffd723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.layers[-2].output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dropout_2/cond/Merge:0\", shape=(?, 1024), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TvFvf1oAmn4",
        "colab_type": "code",
        "outputId": "5a4c5cec-dd5f-4a40-90ad-0483d9ae7a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "g = model.get_layer(name='dropout_2', index=None).output\n",
        "tf.Print(g, [tf.shape(g)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Print_5:0' shape=(?, 1024) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "27a69625-eb71-43fd-eaf6-968fff7b2d78",
        "id": "gPPOo-sDZ1Z-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djj6O2U81ZHH",
        "colab_type": "text"
      },
      "source": [
        "# For vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq78J_ZIn38P",
        "colab_type": "code",
        "outputId": "9b6d459e-fad6-4a4d-c3f8-118f8fde7636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# Load the VGG model\n",
        "pre_trained_model = VGG16(input_shape = (224, 224, 3), \n",
        "                                include_top = False)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmzA4Js_o4-0",
        "colab_type": "code",
        "outputId": "90dcab64-bbe7-408b-c638-e8b848781bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Freeze the layers except the last 1 layers\n",
        "#for layer in pre_trained_model.layers[:-4]:\n",
        "#  layer.trainable = False\n",
        "  \n",
        "# Check the trainable status of the individual layers\n",
        "for layer in pre_trained_model.layers:\n",
        "  print(layer,layer.trainable)\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7fa2492b2710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20ea415f8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20ea41630> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1ea518> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1f8f60> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1fbe80> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e203c50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e190b00> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e196358> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e19b1d0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1a0b70> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1aea20> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1b42b0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1b95c0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1c0a20> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1cc940> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e152208> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e157588> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1607f0> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxMLsbQCp0yg",
        "colab_type": "code",
        "outputId": "1747f773-ae8a-403d-dc0c-e24582701427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        " \n",
        "# Create the model\n",
        "model = models.Sequential()\n",
        " \n",
        "# Add the vgg convolutional base model\n",
        "model.add(pre_trained_model)\n",
        " \n",
        "# Add new layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        " \n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              25691136  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 40,406,849\n",
            "Trainable params: 40,406,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvlmlxwDtD4C",
        "colab_type": "text"
      },
      "source": [
        "# Dataset from Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1usVCS6mlzM3",
        "colab_type": "code",
        "outputId": "8c6834be-07f8-4617-a3e8-4cf795019afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtPEZEhEtEDV",
        "colab_type": "code",
        "outputId": "a178add4-eb0c-4543-8bff-99dd53082d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/Cocoon Dataset.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/content/Cocoon Dataset'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'Train' )\n",
        "validation_dir = os.path.join( base_dir, 'Test')\n",
        "\n",
        "\n",
        "train_Male_dir = os.path.join(train_dir, 'Male') # Directory with our training cat pictures\n",
        "train_Female_dir = os.path.join(train_dir, 'Female') # Directory with our training dog pictures\n",
        "validation_Male_dir = os.path.join(validation_dir, 'Male') # Directory with our validation cat pictures\n",
        "validation_Female_dir = os.path.join(validation_dir, 'Female')# Directory with our validation dog pictures\n",
        "\n",
        "train_Male_fnames = os.listdir(train_Male_dir)\n",
        "train_Female_fnames = os.listdir(train_Female_dir)\n",
        "validation_Male_fnames = os.listdir(validation_Male_dir)\n",
        "validation_Female_fnames = os.listdir(validation_Female_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 21,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (224, 224))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 10,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (224, 224))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 357 images belonging to 2 classes.\n",
            "Found 90 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5eN2wkrtHPr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llDiaK5rtHf0",
        "colab_type": "code",
        "outputId": "8efcf803-c6c1-4299-f549-89b1dccba2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer = optimizers.adam(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch = train_generator.samples/train_generator.batch_size ,\n",
        "            epochs = 30,\n",
        "            validation_data = validation_generator,\n",
        "            validation_steps = validation_generator.samples/validation_generator.batch_size,\n",
        "            verbose = 1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "17/17 [==============================] - 109s 6s/step - loss: 0.7832 - acc: 0.4594 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 2/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6884 - acc: 0.5686 - val_loss: 0.6869 - val_acc: 0.5556\n",
            "Epoch 3/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6895 - acc: 0.5714 - val_loss: 0.6928 - val_acc: 0.5556\n",
            "Epoch 4/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6978 - acc: 0.5462 - val_loss: 0.6880 - val_acc: 0.5556\n",
            "Epoch 5/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6910 - acc: 0.5602 - val_loss: 0.6880 - val_acc: 0.5556\n",
            "Epoch 6/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6892 - acc: 0.5686 - val_loss: 0.6876 - val_acc: 0.5556\n",
            "Epoch 7/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6867 - acc: 0.5714 - val_loss: 0.6882 - val_acc: 0.5556\n",
            "Epoch 8/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6866 - acc: 0.5714 - val_loss: 0.6871 - val_acc: 0.5556\n",
            "Epoch 9/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6862 - acc: 0.5630 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 10/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6837 - acc: 0.5686 - val_loss: 0.6891 - val_acc: 0.5556\n",
            "Epoch 11/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6868 - acc: 0.5686 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 12/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6858 - acc: 0.5686 - val_loss: 0.6883 - val_acc: 0.5556\n",
            "Epoch 13/30\n",
            "17/17 [==============================] - 103s 6s/step - loss: 0.6885 - acc: 0.5686 - val_loss: 0.6873 - val_acc: 0.5556\n",
            "Epoch 14/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6846 - acc: 0.5658 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 15/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6849 - acc: 0.5686 - val_loss: 0.6869 - val_acc: 0.5556\n",
            "Epoch 16/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6869 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 17/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6842 - acc: 0.5686 - val_loss: 0.6891 - val_acc: 0.5556\n",
            "Epoch 18/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6869 - acc: 0.5686 - val_loss: 0.6871 - val_acc: 0.5556\n",
            "Epoch 19/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 20/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6845 - acc: 0.5686 - val_loss: 0.6882 - val_acc: 0.5556\n",
            "Epoch 21/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6816 - acc: 0.5686 - val_loss: 0.6883 - val_acc: 0.5556\n",
            "Epoch 22/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6791 - acc: 0.5686 - val_loss: 0.6909 - val_acc: 0.5556\n",
            "Epoch 23/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.7002 - acc: 0.5602 - val_loss: 0.6875 - val_acc: 0.5556\n",
            "Epoch 24/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6897 - acc: 0.5686 - val_loss: 0.6872 - val_acc: 0.5556\n",
            "Epoch 25/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6926 - acc: 0.5658 - val_loss: 0.6907 - val_acc: 0.5556\n",
            "Epoch 26/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6883 - acc: 0.5686 - val_loss: 0.6872 - val_acc: 0.5556\n",
            "Epoch 27/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6874 - acc: 0.5686 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 28/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6859 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 29/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6870 - acc: 0.5686 - val_loss: 0.6871 - val_acc: 0.5556\n",
            "Epoch 30/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6865 - acc: 0.5686 - val_loss: 0.6873 - val_acc: 0.5556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0HrEb4itKxY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atWMdc4QtK7f",
        "colab_type": "code",
        "outputId": "b8f8451a-1ccf-4796-d7d0-341b1b528073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcFNW5//HPw7DKvrmBCiqKbIMw\nAkZAUSGYKEZREUFFg6gRMd5ogolb9OqNa3DhejWKigvLzxUiSlDxhcSgDMgioEKUBBBxWGSRzYHn\n98epmTTDLD0zPfRM9/f9evVruqpOVT3V1fPU6VNVp8zdERGR9FAt2QGIiMiBo6QvIpJGlPRFRNKI\nkr6ISBpR0hcRSSNK+iIiaURJPw2ZWYaZbTOzIxNZNpnM7FgzS/j1x2Z2ppmtjBn+wsx6xVO2DOt6\n2sx+X9b5ReJRPdkBSMnMbFvM4EHALmBPNHy1u79UmuW5+x6gXqLLpgN3Pz4RyzGz4cBQdz8tZtnD\nE7FskeIo6VcB7p6fdKOa5HB3f7eo8mZW3d1zD0RsIiXR97FyUfNOCjCz/zazSWY2wcy2AkPN7GQz\nm2Nm35vZWjN71MxqROWrm5mbWato+MVo+ttmttXM/mFmrUtbNpp+lpl9aWabzewxM/u7mQ0rIu54\nYrzazFaY2SYzezRm3gwz+7OZbTCzr4D+xXw+fzCziQXGjTWzh6P3w81sWbQ9/4xq4UUta7WZnRa9\nP8jMXohiWwJ0LVD2VjP7KlruEjMbEI3vCDwO9IqaztbHfLZ3xsx/TbTtG8zsDTM7LJ7PpjSfc148\nZvaumW00s2/N7Lcx67kt+ky2mFm2mR1eWFOamc3O28/R5zkrWs9G4FYza2NmM6N1rI8+t4Yx8x8V\nbWNONP0RM6sdxXxCTLnDzGy7mTUtanulBO6uVxV6ASuBMwuM+29gN3AO4UBeBzgJ6E74NXc08CUw\nMipfHXCgVTT8IrAeyAJqAJOAF8tQ9mBgK3BuNO2/gB+BYUVsSzwxvgk0BFoBG/O2HRgJLAFaAk2B\nWeHrXOh6jga2AXVjlv0dkBUNnxOVMeB0YAfQKZp2JrAyZlmrgdOi9w8CHwCNgaOApQXKXgQcFu2T\nS6IYDommDQc+KBDni8Cd0ft+UYydgdrA/wLvx/PZlPJzbgisA24AagENgG7RtFuAhUCbaBs6A02A\nYwt+1sDsvP0cbVsucC2QQfg+HgecAdSMvid/Bx6M2Z7Pos+zblT+lGjaU8A9Mev5DfB6sv8Pq/Ir\n6QHoVcodVnTSf7+E+W4C/l/0vrBE/n8xZQcAn5Wh7JXAhzHTDFhLEUk/zhh7xEx/Dbgpej+L0MyV\nN+1nBRNRgWXPAS6J3p8FfFFM2b8C10Xvi0v6/47dF8CvYssWstzPgJ9H70tK+s8D98ZMa0A4j9Oy\npM+mlJ/zpcDcIsr9My/eAuPjSfpflRDDBXnrBXoB3wIZhZQ7BfgasGh4AXB+ov+v0uml5p3UsSp2\nwMzamtlb0c/1LcBdQLNi5v825v12ij95W1TZw2Pj8PBfurqohcQZY1zrAv5VTLwALwODo/eXRMN5\ncZxtZh9HTQ/fE2rZxX1WeQ4rLgYzG2ZmC6Mmiu+BtnEuF8L25S/P3bcAm4AWMWXi2mclfM5HEJJ7\nYYqbVpKC38dDzWyyma2JYniuQAwrPVw0sA93/zvhV0NPM+sAHAm8VcaYBLXpp5KClys+SahZHuvu\nDYDbCTXvirSWUBMFwMyMfZNUQeWJcS0hWeQp6ZLSycCZZtaC0Pz0chRjHeAV4H8ITS+NgL/FGce3\nRcVgZkcDTxCaOJpGy/08ZrklXV76DaHJKG959QnNSGviiKug4j7nVcAxRcxX1LQfopgOihl3aIEy\nBbfvPsJVZx2jGIYViOEoM8soIo7xwFDCr5LJ7r6riHISByX91FUf2Az8EJ0Iu/oArPOvQBczO8fM\nqhPaiZtXUIyTgV+bWYvopN7viivs7t8SmiCeIzTtLI8m1SK0M+cAe8zsbELbc7wx/N7MGlm4j2Fk\nzLR6hMSXQzj+XUWo6edZB7SMPaFawATgl2bWycxqEQ5KH7p7kb+cilHc5zwFONLMRppZLTNrYGbd\nomlPA/9tZsdY0NnMmhAOdt8SLhjIMLMRxBygionhB2CzmR1BaGLK8w9gA3CvhZPjdczslJjpLxCa\ngy4hHACkHJT0U9dvgMsJJ1afJJxwrVDuvg4YBDxM+Cc+BviUUMNLdIxPAO8Bi4G5hNp6SV4mtNHn\nN+24+/fAjcDrhJOhFxAOXvG4g/CLYyXwNjEJyd0XAY8Bn0Rljgc+jpl3BrAcWGdmsc00efO/Q2iG\neT2a/0hgSJxxFVTk5+zum4G+wEDCgehL4NRo8gPAG4TPeQvhpGrtqNnuKuD3hJP6xxbYtsLcAXQj\nHHymAK/GxJALnA2cQKj1/5uwH/KmryTs513u/lEpt10KyDs5IpJw0c/1b4AL3P3DZMcjVZeZjSec\nHL4z2bFUdbo5SxLKzPoTrpTZQbjk70dCbVekTKLzI+cCHZMdSypQ844kWk/gK0Jb9k+B83TiTcrK\nzP6HcK/Ave7+72THkwrUvCMikkZU0xcRSSOVrk2/WbNm3qpVq2SHISJSpcybN2+9uxd3iTRQCZN+\nq1atyM7OTnYYIiJVipmVdFc6oOYdEZG0oqQvIpJGlPRFRNKIkr6ISBpR0hcRSSNK+iIiaURJX0Qk\njVS66/QrlZwceOUVGDQImjRJdjRF++47eOEF2Lw5vvLdusHZZ1dsTCJSKSnpF+Zf/4KHHoKnn4Yd\nO0Lyv/32ZEe1v5Ur4cEH4ZlnYOdOsDge9uQeys2cCaeeWnJ5EUkpat6JtWQJXH45HHssPPEEXHxx\neD97drIj29dnn8HQoSG2p54K77/4AvbuLfm1dSsccwxceil8/32yt0REDjAlfYA5c+AXv4AOHUJz\nzsiR8NVXMG4c9OsH//gH5OYmO0r46CM45xzo2BHeeANuuAG+/hr+8hc47rj4llGvHrz0EnzzDVx7\nbaj5i0jaSN+k7w7Tp0OfPnDyyfDhh3DHHfDvf8Of/wxHRM+77tkTtm2DhQuTF+fbb0Pv3nDKKeEA\n9Mc/hjgfeghaFPfc8SJ06xaWMXFiOACISNpIzzb9JUvgsstg/vyQNB9+GK66KtSCC+rVK/ydPRu6\ndj2wcX70EfzqV+GA07IljBkDw4dD3brlX/bo0fDOO3DddeHAVp6eTWfNgldfLblcRaleHQYOhJ/8\nJDHLW7ECnnsuNIVJ8U4/Hc49N3HLe//90FR52WWJ+Z7v2gUvvwwLFpR/WQfCUUfBf/1Xha6i0j1E\nJSsryyu8l80RI0IN97HHQnt4zZrFl2/VCrKyQtPPgbJuXWjGqVMH7rwThgwpOc7SWrkSMjPDej74\nICTP0nrlFbjkkjBvrVqJjS9eO3aEf+7evcPBrH//+E5qF/Tpp3DfffD//h9Uq1Z4JUD+Y/fucAHB\n7Nnh13J5/fOf0Llz+GXdpAmMGhWaWps2Lf2ytm0L57sefhjWrIH69SEjo/wxVrSsLJgxo0yzmtk8\nd88qsaC7V6pX165dvcJddJF727bxlx861P2QQ9z37q24mGLt3ev+s5+516rl/tlnFbuuF15wB/e7\n7y79vOPGuVer5t6zp/v33yc+tnht2+b+yCPuRxwRtiUz033CBPcffyx53r173T/4wL1//zBv/fru\nv/ud+9q1FR93Vbd5s3vr1uG1eXP5lvXjj+49erg3auT++uvu55wT9kfduu433ui+alV8y8nJcb/9\ndvfGjcP8ffq4/+1vB+5/N4mAbI8jxyY9yRd8HZCk37+/e7du8Zf/v/8LH9Xy5RUXU6yxY8P6Hn20\n4te1d6/74MHuGRnuH38c/3xjxoQYf/pT9x9+qLj4SmPXLvfnnnM/4YQQ2zHHhH23Y8f+ZffscX/z\nTfeTTw5lDz7Y/d573TdtOvBxV2WzZ4cD/+WXl285d9wR9sPEif8Zt3hxqHBlZLjXqOF+5ZXun39e\n+Pz/+pf7DTe4H3RQWM4vfuE+Z075YqpilPSL85OfuJ95ZvzlP/ssfFTjxlVcTHmWLHGvXTscmA5U\n7WTTJvcjj3Q/9lj3rVuLL7t3r/tdd4XPY+BA9507D0yMpbFnT6gtdusW4jz0UPf77gu10d273ceP\nd2/fPkxr1SocZLdvT3bUVddtt4XPcvLkss3/0UfhwHHppYVP//pr9+uuC/8XZuF7N3dumLZ0qfuw\nYe7Vq4fX5ZeHcWlISb84HTq4n39+/OX37HFv0iTUNCrSzp3unTu7N2t24JsXPvgg/EMNH150mb17\n3X/zm/C1GTYsvuaTZNq71/2999z79g0xN2wYDm4QvgMvvlj5t6Eq2L07HGAbNXL/979LN++WLe5H\nHx0OviU1Ea1b5/6HP4T9mLcPzdzr1HEfNcp95cqyb0MKUNIvzpFHhqRVGuec496mTcXEk+fmm8Mu\nefPNil1PUUaPDut/7bX9p+Xmul91VZg+alQ4EFYl2dnhXE7fvu5Tp1a9+Cu75ctD+3ufPqX7bIcN\nC7X82bPjn2fzZvf773c/6aTwK+O770ofbwpS0i9Oo0YhcZXG/feHj+vbbysmpvffD7WWq6+umOXH\nY9cu9y5dwq+aNWv+M373bvdBg8L233ZbWpwUkzJ45pnwHbn//vjKT578n++UlFu8ST/9bs5yhy1b\noEGD0s3Xs2f4+/e/Jz6mjRtDtwht2oQbrpKlZs1wTfOOHTBsWOi2YccOOO88mDQJHngA7rqrbJdD\nSuq74go4/3z4wx/CPTDFWb0arr463Ch4220HJj4B0vGO3O3bQzIrbdLv2hVq1w537iaSO1xzTbgu\n/+WXE3NDSnkcf3y4I3nGDLj3XjjrLJg2DZ58Em66KbmxSeVmFq6Nb9483FeyfXvh5fbuDTdf7d4d\n7pepUePAxpnm0i/pb9kS/pY26desCd27J77ztfHjw81Ad9994O/4LcqIETBgQKiB/f3v4R9zxIhk\nRyVVQdOm8Pzz8PnncPPNhZd56KHQy+ujj4ZOA+WASt+kX79+6eft2TPctbltW2Ji+eqrcMdh795F\n/4Mkg1noVvqcc0LHboMHJzsiqUrOPDN0JfC//wt//eu+0z79NDT/nH9+aA6SAy59k35pa/oQ+uHZ\nsyf0ylleubmhC4iMjPAAlMp2i3jz5jBlCvz858mORKqie++FTp3gyitD0yWE5p4hQ8J366mndG4o\nSeJK+mbW38y+MLMVZja6kOnDzCzHzBZEr+Ex0440s7+Z2TIzW2pmrRIXfhmUJ+mffHLokyURTTz3\n3ht6zHziCTjyyPIvT6QyqVUrnKPasiUkfnf47W9h2bLQmV1Z+tORhCixhy0zywDGAn2B1cBcM5vi\n7ksLFJ3k7iMLWcR44B53n2Fm9YC95Q26XMqT9Bs0CB2Ulfdk7pw54SqYoUPVdCKpq337cMXXqFGh\nhj9hQmj26ds32ZGltXhq+t2AFe7+lbvvBiYCcfWlambtgOruPgPA3be5exGn9A+Q8iR9CO36c+bA\njz+Wbf6dO0Oyb9kSHn+8bMsQqSpGjgy9nk6YEJp77r032RGlvXiSfgtgVczw6mhcQQPNbJGZvWJm\n0RNIOA743sxeM7NPzeyB6JfDPsxshJllm1l2Tk5OqTeiVBKR9LdvDyekymLixNCF7JNPQsOGZVuG\nSFVhBs8+G+5DmTQped1vS75EncidCrRy907ADOD5aHx1oBdwE3AScDQwrODM7v6Uu2e5e1bz5s0T\nFFIRynP1DvznJq2ytOu7hz7827cPj2EUSQeHHhouTW7bNtmRCPEl/TXAETHDLaNx+dx9g7vvigaf\nBvIuOF8NLIiahnKBN4Au5Qu5nLZuDbWNstY4Dj8cjj66bO36c+aEOxVHjtSVCyKSFPEk/blAGzNr\nbWY1gYuBKbEFzOywmMEBwLKYeRuZWV71/XSg4AngA6ssXTAU1KtXqOl7KZ869thjoUln6NDyrV9E\npIxKTPpRDX0kMJ2QzCe7+xIzu8vMBkTFRpnZEjNbCIwiasJx9z2Epp33zGwxYMBfEr8ZpZCIpN+z\nJ6xfD19+Gf88a9eGO2+vuEKP4RORpInroajuPg2YVmDc7THvbwFuKWLeGUCncsSYWImq6UNo4jn+\n+PjmefLJcGPXddeVb90iIuWQnnfkljfpH3ccNGsW/8nc3btD0j/rLPU1IiJJFVdNv6r49a9hwYIS\nCs17MJzEPa08azLwmTBpG6yMo/h3m+DbCbCqUznXKyKprHNnGDOmYteRfjX93D2QkYBjXaOG4Uar\n3btKLrtmDdSpA00al3+9IiLlkFI1/biOkAefAwMvCD0Alscna6F7H7hxMlx4YdHl5s2DrKwQ3A3d\ny7dOEZFySr+afiLa9AFOPBEOOqjk6/Uffzw8GGXYsPKvU0SknNIr6e/aFV6JSPo1akCPHsWfzM3J\nCX2OXH65ulwQkUohvZL+1q3hb1m7YCioVy9YuPA/XTsU9PTT4SCjyzRFpJJIr6Rf3s7WCurZMzzv\n8x//2H9abm7oK/+MM6Bdu8SsT0SknJT0y6NHj/DEq8KaeKZMgVWr4PrrE7MuEZEEUNIvj3r1wgnd\nwk7mPvYYHHUUnH12YtYlIpIASvrl1bMnfPxxuOs2z+LF8MEH8KtfVb5n34pIWlPSL69evcJNWvPm\n/Wfc449D7drwy18mbj0iIgmQXkk/7+qdRCb9U04Jf/Pa9TdtghdfDM8E1cOfRaSSSa+kXxE1/UMO\nCR2w5SX9Z58Nj1McWdgz4kVEkiv9kn61auFO2kTq2TMk/dxcGDs2DHfunNh1iIgkQPol/QYNEv+o\nwp49YeNGePhh+OorXaYpIpVWeib9RMt7qMrtt0OLFnDeeYlfh4hIAijpJ8Ixx4S2/V274JprQr88\nIiKVkJJ+IphB795QsyZcdVXily8ikiBK+onyP/8Db70VavwiIpVUSj1EpURbtkCrVhWz7GOOCS8R\nkUpMNX0RkTSipC8ikkbSJ+nv2QPbtiXuASoiIlVQ+iT9bdvCX9X0RSSNpU/Sr4h+d0REqhglfRGR\nNBJX0jez/mb2hZmtMLPRhUwfZmY5ZrYgeg0vML2Bma02s8cTFXipKemLiJR8nb6ZZQBjgb7AamCu\nmU1x96UFik5y96L6E74bmFWuSMtLSV9EJK6afjdghbt/5e67gYnAufGuwMy6AocAfytbiAlSEQ9Q\nERGpYuJJ+i2AVTHDq6NxBQ00s0Vm9oqZHQFgZtWAh4CbiluBmY0ws2wzy87JyYkz9FJSTV9EJGEn\ncqcCrdy9EzADeD4a/ytgmruvLm5md3/K3bPcPat58+YJCqkAJX0Rkbj63lkDHBEz3DIal8/dN8QM\nPg3cH70/GehlZr8C6gE1zWybu+93MrjC5SV93ZwlImksnqQ/F2hjZq0Jyf5i4JLYAmZ2mLuvjQYH\nAMsA3H1ITJlhQFZSEj6EpF+3LmRkJGX1IiKVQYlJ391zzWwkMB3IAMa5+xIzuwvIdvcpwCgzGwDk\nAhuBYRUYc9mo3x0Rkfi6Vnb3acC0AuNuj3l/C3BLCct4Dniu1BEmipK+iEia3ZGrpC8iaU5JX0Qk\njSjpi4ikESV9EZE0kl5JX9foi0iaS4+k766avogI6ZL0d+wIj0tU0heRNJceSV/97oiIAEr6IiJp\nJT2SvvrSFxEB0iXpq6YvIgIo6YuIpBUlfRGRNKKkLyKSRpT0RUTSSPok/Zo1oVatZEciIpJU6ZP0\nVcsXEVHSFxFJJ0r6IiJpRElfRCSNpE/SV1/6IiJplPRV0xcRUdIXEUknSvoiImkk9ZP+7t2wc6eS\nvogI6ZD01Ze+iEi+uJK+mfU3sy/MbIWZjS5k+jAzyzGzBdFreDS+s5n9w8yWmNkiMxuU6A0okZK+\niEi+6iUVMLMMYCzQF1gNzDWzKe6+tEDRSe4+ssC47cBl7r7czA4H5pnZdHf/PhHBx0WdrYmI5Iun\npt8NWOHuX7n7bmAicG48C3f3L919efT+G+A7oHlZgy0TJX0RkXzxJP0WwKqY4dXRuIIGRk04r5jZ\nEQUnmlk3oCbwz0KmjTCzbDPLzsnJiTP0OCnpi4jkS9SJ3KlAK3fvBMwAno+daGaHAS8AV7j73oIz\nu/tT7p7l7lnNmyf4h4CSvohIvniS/hogtubeMhqXz903uPuuaPBpoGveNDNrALwF/MHd55Qv3DJQ\n0hcRyRdP0p8LtDGz1mZWE7gYmBJbIKrJ5xkALIvG1wReB8a7+yuJCbmUlPRFRPKVePWOu+ea2Uhg\nOpABjHP3JWZ2F5Dt7lOAUWY2AMgFNgLDotkvAnoDTc0sb9wwd1+Q2M0oxpYtYAZ16x6wVYqIVFYl\nJn0Ad58GTCsw7vaY97cAtxQy34vAi+WMsXzyumAwS2oYIiKVQerfkat+d0RE8inpi4ikkfRI+nqA\niogIkC5JXzV9ERFASV9EJK0o6YuIpBElfRGRNJLaSX/vXti2TUlfRCSS2kn/hx/AXUlfRCSS2klf\n/e6IiOxDSV9EJI0o6YuIpBElfRGRNKKkLyKSRpT0RUTSiJK+iEgaSY+kr142RUSAdEj6Bx0E1eN6\nQJiISMpL/aSvph0RkXypn/TVtCMiki/1k75q+iIi+ZT0RUTSiJK+iEgaUdIXEUkjqZ30t25V0hcR\niZG6Sd9dNX0RkQLiSvpm1t/MvjCzFWY2upDpw8wsx8wWRK/hMdMuN7Pl0evyRAZfrF274McflfRF\nRGKUeKuqmWUAY4G+wGpgrplNcfelBYpOcveRBeZtAtwBZAEOzIvm3ZSQ6IujfndERPYTT02/G7DC\n3b9y993ARODcOJf/U2CGu2+MEv0MoH/ZQi0lJX0Rkf3Ek/RbAKtihldH4woaaGaLzOwVMzuiNPOa\n2Qgzyzaz7JycnDhDL4GSvojIfhJ1Incq0MrdOxFq88+XZmZ3f8rds9w9q3nz5omJSElfRGQ/8ST9\nNcARMcMto3H53H2Du++KBp8GusY7b4VR0hcR2U88SX8u0MbMWptZTeBiYEpsATM7LGZwALAsej8d\n6Gdmjc2sMdAvGlfxlPRFRPZT4tU77p5rZiMJyToDGOfuS8zsLiDb3acAo8xsAJALbASGRfNuNLO7\nCQcOgLvcfWMFbMf+lPRFRPYT19NF3H0aMK3AuNtj3t8C3FLEvOOAceWIsWyU9EVE9pO6d+Ru2QI1\nakCtWsmORESk0kjtpF+/PpglOxIRkUojtZO+mnZERPahpC8ikkaU9EVE0kjqJn31pS8isp/UTfqq\n6YuI7EdJX0QkjSjpi4ikkdRM+rm5sH27kr6ISAGpmfS3bg1/lfRFRPaRmklf/e6IiBRKSV9EJI0o\n6YuIpBElfRGRNKKkLyKSRpT0RUTSiJK+iEgaSe2kX7ducuMQEalkUjfp168P1VJz80REyio1s6L6\n3RERKZSSvohIGknNpK8HqIiIFCo1k75q+iIihVLSFxFJI0r6IiJpJK6kb2b9zewLM1thZqOLKTfQ\nzNzMsqLhGmb2vJktNrNlZnZLogIvlpK+iEihSkz6ZpYBjAXOAtoBg82sXSHl6gM3AB/HjL4QqOXu\nHYGuwNVm1qr8YRfDXUlfRKQI8dT0uwEr3P0rd98NTATOLaTc3cB9wM6YcQ7UNbPqQB1gN7ClfCGX\n4IcfQuJX0hcR2U88Sb8FsCpmeHU0Lp+ZdQGOcPe3Csz7CvADsBb4N/Cgu28se7hxUL87IiJFKveJ\nXDOrBjwM/KaQyd2APcDhQGvgN2Z2dCHLGGFm2WaWnZOTU76AlPRFRIoUT9JfAxwRM9wyGpenPtAB\n+MDMVgI9gCnRydxLgHfc/Ud3/w74O5BVcAXu/pS7Z7l7VvPmzcu2JXmU9EVEihRP0p8LtDGz1mZW\nE7gYmJI30d03u3szd2/l7q2AOcAAd88mNOmcDmBmdQkHhM8TvA37UtIXESlSiUnf3XOBkcB0YBkw\n2d2XmNldZjaghNnHAvXMbAnh4PGsuy8qb9DFUtIXESlS9XgKufs0YFqBcbcXUfa0mPfbCJdtHjh5\nSb9+/QO6WhGRqiD17shVTV9EpEipm/RV0xcR2U9qJv3ataFmzWRHIiJS6cTVpl+lqC99SSE//vgj\nq1evZufOnSUXlrRQu3ZtWrZsSY0aNco0f+olffW7Iylk9erV1K9fn1atWmFmyQ5Hkszd2bBhA6tX\nr6Z169ZlWkZqNu8o6UuK2LlzJ02bNlXCFwDMjKZNm5brl5+Svkglp4Qvscr7fVDSFxFJI0r6IlKk\nDRs20LlzZzp37syhhx5KixYt8od3794d1zKuuOIKvvjii2LLjB07lpdeeikRIUsJdCJXRIrUtGlT\nFixYAMCdd95JvXr1uOmmm/Yp4+64O9WqFV6HfPbZZ0tcz3XXXVf+YA+w3NxcqleveilUNX2RquLX\nv4bTTkvs69e/LlMoK1asoF27dgwZMoT27duzdu1aRowYQVZWFu3bt+euu+7KL9uzZ08WLFhAbm4u\njRo1YvTo0WRmZnLyySfz3XffAXDrrbcyZsyY/PKjR4+mW7duHH/88Xz00UcA/PDDDwwcOJB27dpx\nwQUXkJWVlX9AinXHHXdw0kkn0aFDB6655hrcHYAvv/yS008/nczMTLp06cLKlSsBuPfee+nYsSOZ\nmZn84Q9/2CdmgG+//ZZjjz0WgKeffppf/OIX9OnTh5/+9Kds2bKF008/nS5dutCpUyf++te/5sfx\n7LPP0qlTJzIzM7niiivYvHkzRx99NLm5uQBs2rRpn+EDJbWS/q5dsHu3kr7IAfD5559z4403snTp\nUlq0aMGf/vQnsrOzWbhwITNmzGDp0qX7zbN582ZOPfVUFi5cyMknn8y4ceMKXba788knn/DAAw/k\nH0Aee+wxDj30UJYuXcptt93Gp59+Wui8N9xwA3PnzmXx4sVs3ryZd955B4DBgwdz4403snDhQj76\n6CMOPvhgpk6dyttvv80nn3zCwoUL+c1vCnssyL4+/fRTXnvtNd577z3q1KnDG2+8wfz583n33Xe5\n8cYbAVi4cCH33XcfH3zwAQsXLuShhx6iYcOGnHLKKfnxTJgwgQsvvPCA/1qoer9NiqN+dySVRTXh\nyuKYY44hK+s/j8eYMGECzzyMsJW3AAAO+ElEQVTzDLm5uXzzzTcsXbqUdu32fZx2nTp1OOusswDo\n2rUrH374YaHLPv/88/PL5NXIZ8+eze9+9zsAMjMzad++faHzvvfeezzwwAPs3LmT9evX07VrV3r0\n6MH69es555xzgHCDE8C7777LlVdeSZ06dQBo0qRJidvdr18/GjduDISD0+jRo5k9ezbVqlVj1apV\nrF+/nvfff59BgwblLy/v7/Dhw3n00Uc5++yzefbZZ3nhhRdKXF+iKemLSJnUrVs3//3y5ct55JFH\n+OSTT2jUqBFDhw4t9FrymjHdo2RkZBTZtFGrVq0SyxRm+/btjBw5kvnz59OiRQtuvfXWMl3TXr16\ndfbu3Quw3/yx2z1+/Hg2b97M/PnzqV69Oi1btix2faeeeiojR45k5syZ1KhRg7Zt25Y6tvJKreYd\nJX2RpNiyZQv169enQYMGrF27lunTpyd8HaeccgqTJ08GYPHixYU2H+3YsYNq1arRrFkztm7dyquv\nvgpA48aNad68OVOnTgVCIt++fTt9+/Zl3Lhx7NixA4CNG8MjvFu1asW8efMAeOWVV4qMafPmzRx8\n8MFUr16dGTNmsGZNeKjg6aefzqRJk/KXl/cXYOjQoQwZMoQrrriiXJ9HWSnpi0i5denShXbt2tG2\nbVsuu+wyTjnllISv4/rrr2fNmjW0a9eOP/7xj7Rr146GDRvuU6Zp06ZcfvnltGvXjrPOOovu3bvn\nT3vppZd46KGH6NSpEz179iQnJ4ezzz6b/v37k5WVRefOnfnzn/8MwM0338wjjzxCly5d2LRpU5Ex\nXXrppXz00Ud07NiRiRMn0qZNGyA0P/32t7+ld+/edO7cmZtvvjl/niFDhrB582YGDRqUyI8nbpZ3\nZruyyMrK8uzs7LLNPHUqDBgAc+dC1n6P4hWpcpYtW8YJJ5yQ7DAqhdzcXHJzc6lduzbLly+nX79+\nLF++vMpdNjlx4kSmT58e16WsRSnse2Fm89y9xMRXtT6tkqimL5Kytm3bxhlnnEFubi7uzpNPPlnl\nEv61117Lu+++m38FTzJUrU+sJEr6IimrUaNG+e3sVdUTTzyR7BDUpi8ikk5SK+lv3QoZGRBdcysi\nIvtKraSf1wWDuqIVESlUaiZ9EREplJK+iBSpT58++91oNWbMGK699tpi56tXrx4A33zzDRdccEGh\nZU477TRKujx7zJgxbN++PX/4Zz/7Gd9//308oUsRlPRFpEiDBw9m4sSJ+4ybOHEigwcPjmv+ww8/\nvNg7WktSMOlPmzaNRo0alXl5B5q753fnUFko6YtUEcnoWfmCCy7grbfeyn9gysqVK/nmm2/o1atX\n/nXzXbp0oWPHjrz55pv7zb9y5Uo6dOgAhC4SLr74Yk444QTOO++8/K4PIFy/ntct8x133AHAo48+\nyjfffEOfPn3o06cPELpHWL9+PQAPP/wwHTp0oEOHDvndMq9cuZITTjiBq666ivbt29OvX7991pNn\n6tSpdO/enRNPPJEzzzyTdevWAeFegCuuuIKOHTvSqVOn/G4c3nnnHbp06UJmZiZnnHEGEJ4v8OCD\nD+Yvs0OHDqxcuZKVK1dy/PHHc9lll9GhQwdWrVpV6PYBzJ07l5/85CdkZmbSrVs3tm7dSu/evffp\nMrpnz54sXLiw+B1VCql3nf7RRyc7CpGU0aRJE7p168bbb7/Nueeey8SJE7noooswM2rXrs3rr79O\ngwYNWL9+PT169GDAgAFFPsP1iSee4KCDDmLZsmUsWrSILl265E+75557aNKkCXv27OGMM85g0aJF\njBo1iocffpiZM2fSrFmzfZY1b948nn32WT7++GPcne7du3PqqafSuHFjli9fzoQJE/jLX/7CRRdd\nxKuvvsrQoUP3mb9nz57MmTMHM+Ppp5/m/vvv56GHHuLuu++mYcOGLF68GAh93ufk5HDVVVcxa9Ys\nWrduvU8/OkVZvnw5zz//PD169Chy+9q2bcugQYOYNGkSJ510Elu2bKFOnTr88pe/5LnnnmPMmDF8\n+eWX7Ny5k8zMzFLtt+LElfTNrD/wCJABPO3ufyqi3EDgFeAkd8+OxnUCngQaAHujaWV/lHtxVNOX\nFJasnpXzmnjykv4zzzwDhKaL3//+98yaNYtq1aqxZs0a1q1bx6GHHlrocmbNmsWoUaMA6NSpE506\ndcqfNnnyZJ566ilyc3NZu3YtS5cu3Wd6QbNnz+a8887L7/Hy/PPP58MPP2TAgAG0bt2azp07A/t2\nzRxr9erVDBo0iLVr17J7925at24NhK6WY5uzGjduzNSpU+ndu3d+mXi6Xz7qqKPyE35R22dmHHbY\nYZx00kkANIhy14UXXsjdd9/NAw88wLhx4xg2bFiJ6yuNEpt3zCwDGAucBbQDBptZu0LK1QduAD6O\nGVcdeBG4xt3bA6cBPyYk8sIo6Ysk3Lnnnst7773H/Pnz2b59O127dgVCB2Y5OTnMmzePBQsWcMgh\nh5SpG+Ovv/6aBx98kPfee49Fixbx85//vEzLyZPXLTMU3TXz9ddfz8iRI1m8eDFPPvlkubtfhn27\nYI7tfrm023fQQQfRt29f3nzzTSZPnsyQIUNKHVtx4mnT7wascPev3H03MBE4t5BydwP3AbFb0w9Y\n5O4LAdx9g7vvKWfMhduzB374QUlfJMHq1atHnz59uPLKK/c5gZvXrXCNGjWYOXMm//rXv4pdTu/e\nvXn55ZcB+Oyzz1i0aBEQumWuW7cuDRs2ZN26dbz99tv589SvX5+tW7fut6xevXrxxhtvsH37dn74\n4Qdef/11evXqFfc2bd68mRYtWgDw/PPP54/v27cvY8eOzR/etGkTPXr0YNasWXz99dfAvt0vz58/\nH4D58+fnTy+oqO07/vjjWbt2LXPnzgVg69at+Qeo4cOHM2rUKE466aT8B7YkSjxJvwWwKmZ4dTQu\nn5l1AY5w97cKzHsc4GY23czmm9lvC1uBmY0ws2wzy87JySlF+DHyvhhK+iIJN3jwYBYuXLhP0h8y\nZAjZ2dl07NiR8ePHl/hAkGuvvZZt27ZxwgkncPvtt+f/YsjMzOTEE0+kbdu2XHLJJft0yzxixAj6\n9++ffyI3T5cuXRg2bBjdunWje/fuDB8+nBNPPDHu7bnzzju58MIL6dq16z7nC2699VY2bdpEhw4d\nyMzMZObMmTRv3pynnnqK888/n8zMzPwukQcOHMjGjRtp3749jz/+OMcdd1yh6ypq+2rWrMmkSZO4\n/vrryczMpG/fvvm/ALp27UqDBg0qps/9vCfZF/UCLiC04+cNXwo8HjNcDfgAaBUNfwBkRe9vAr4G\nmgEHAf8AzihufV27dvUy2bjRfdAg9+nTyza/SCW0dOnSZIcgSbBmzRpv06aN79mzp9DphX0vgGwv\nIZ+7e1w1/TXAETHDLaNxeeoDHYAPzGwl0AOYYmZZhF8Fs9x9vbtvB6YBXagIjRvDxInQr1+FLF5E\n5EAYP3483bt355577qFatcRfVR/PEucCbcystZnVBC4GpuRNdPfN7t7M3Vu5eytgDjDAw9U704GO\nZnZQdFL3VGD/Z5yJiAgAl112GatWreLCCy+skOWXmPTdPRcYSUjgy4DJ7r7EzO4yswElzLsJeJhw\n4FgAzPf92/1FpBheyZ5uJ8lV3u9DXNfpu/s0QtNM7Ljbiyh7WoHhFwmXbYpIKdWuXZsNGzbQtGnT\nIm96kvTh7mzYsIHatWuXeRmpdUeuSIpp2bIlq1evpsxXtUnKqV27Ni1btizz/Er6IpVYjRo18u8E\nFUmE1OpwTUREiqWkLyKSRpT0RUTSiFW2y8HMLAcovhOP4jUD1iconMog1bYHUm+bUm17IPW2KdW2\nB/bfpqPcvXlJM1W6pF9eZpbt7lnJjiNRUm17IPW2KdW2B1Jvm1Jte6Ds26TmHRGRNKKkLyKSRlIx\n6T+V7AASLNW2B1Jvm1JteyD1tinVtgfKuE0p16YvIiJFS8WavoiIFEFJX0QkjaRM0jez/mb2hZmt\nMLPRyY4nEcxspZktNrMFZpad7HhKy8zGmdl3ZvZZzLgmZjbDzJZHfxP7ANAKVsQ23Wlma6L9tMDM\nfpbMGEvDzI4ws5lmttTMlpjZDdH4KrmfitmeqryPapvZJ2a2MNqmP0bjW5vZx1HOmxQ976Tk5aVC\nm76ZZQBfAn0JT+uaCwx29yr9wJboSWRZ7l4lbyoxs97ANmC8u3eIxt0PbHT3P0UH58bu/rtkxlka\nRWzTncA2d38wmbGVhZkdBhzm7vPNrD4wD/gFMIwquJ+K2Z6LqLr7yIC67r7NzGoAs4EbgP8CXnP3\niWb2f8BCd3+ipOWlSk2/G7DC3b9y993ARODcJMeU9tx9FrCxwOhzgeej988T/iGrjCK2qcpy97Xu\nPj96v5XwoKQWVNH9VMz2VFnRI3C3RYM1opcDpwOvROPj3kepkvRbAKtihldTxXd0xIG/mdk8MxuR\n7GAS5BB3Xxu9/xY4JJnBJNBIM1sUNf9UiaaQgsysFXAi8DEpsJ8KbA9U4X1kZhlmtgD4DpgB/BP4\nPnqyIZQi56VK0k9VPd29C3AWcF3UtJAyPLQtVv32RXgCOAboDKwFHkpuOKVnZvWAV4Ffu/uW2GlV\ncT8Vsj1Veh+5+x537wy0JLRstC3rslIl6a8BjogZbhmNq9LcfU309zvgdcLOrurWRe2uee2v3yU5\nnnJz93XRP+Ve4C9Usf0UtRO/Crzk7q9Fo6vsfipse6r6Psrj7t8DM4GTgUZmlvcgrLhzXqok/blA\nm+hsdk3gYmBKkmMqFzOrG52IwszqAv2Az4qfq0qYAlwevb8ceDOJsSREXnKMnEcV2k/RScJngGXu\n/nDMpCq5n4raniq+j5qbWaPofR3CBSvLCMn/gqhY3PsoJa7eAYguwRoDZADj3P2eJIdULmZ2NKF2\nD+Gxli9XtW0yswnAaYQuYNcBdwBvAJOBIwldaF/k7lXmxGgR23QaodnAgZXA1THt4ZWamfUEPgQW\nA3uj0b8ntINXuf1UzPYMpuruo06EE7UZhIr6ZHe/K8oRE4EmwKfAUHffVeLyUiXpi4hIyVKleUdE\nROKgpC8ikkaU9EVE0oiSvohIGlHSFxFJI0r6IiJpRElfRCSN/H/5KUazVxBAZwAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzeW2NZEd7MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSAEkd3qA_m3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c5dbbbff-fe72-4f02-a79f-c6f13e5f4082"
      },
      "source": [
        "validation_male_list = []\n",
        "for name in validation_Male_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Test/Male/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  validation_male_list.append(intermediate_output_list)\n",
        "  \n",
        "validation_male_list = np.array(validation_male_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJPPvsvpLdFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a52ac649-ae97-4dd3-8484-c29d9c706dab"
      },
      "source": [
        "validation_female_list = []\n",
        "for name in validation_Female_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Test/Female/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  validation_female_list.append(intermediate_output_list)\n",
        "  \n",
        "validation_female_list = np.array(validation_female_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFstj1gWL6QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7367160c-24c1-41d6-c07f-c3cb95aa1627"
      },
      "source": [
        "train_male_list = []\n",
        "for name in train_Male_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Train/Male/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  train_male_list.append(intermediate_output_list)\n",
        "  \n",
        "train_male_list = np.array(train_male_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UoTR8WjMKJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0acc8ab6-33c7-41a6-dd7e-c04a13bf1596"
      },
      "source": [
        "train_female_list = []\n",
        "for name in train_Female_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Train/Female/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  train_female_list.append(intermediate_output_list)\n",
        "  \n",
        "train_female_list = np.array(train_female_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "graDVrcLj68c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c9a7dd35-37e6-45bf-965f-daa6969b339d"
      },
      "source": [
        "intermediate_output[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.04516411, 0.18483284, 0.16722398, ..., 0.05645609, 0.25139302,\n",
              "       0.11341704], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXva6MtWc-Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.concatenate((validation_male_list,validation_female_list,train_male_list,train_female_list), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXBxBAbUO3yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(data=data[0: ,0:],\n",
        "            index=[i for i in range(data.shape[0])],\n",
        "            columns=['f'+str(i) for i in range(data.shape[1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2mX7-Sad_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.rename(columns={'f0':'File Name'})\n",
        "df = df.set_index('File Name')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpDrOOcFdojO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "722eac94-44d0-4772-c588-17794c4d234d"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 447 entries, SK777A.jpg to 46B.jpg\n",
            "Columns: 1024 entries, f1 to f1024\n",
            "dtypes: object(1024)\n",
            "memory usage: 3.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4nptduMRN8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "796c5efc-09b9-4bad-af6f-eeea874f4c6a"
      },
      "source": [
        "drop_column = []\n",
        "for i in range(1,1025):\n",
        "  count = 0\n",
        "  for each in df['f' + str(i)]:\n",
        "    if float(each)<= 0.0:\n",
        "      count +=1\n",
        "  if count == len(df):\n",
        "    drop_column.append('f' + str(i))\n",
        "\n",
        "len(drop_column)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "531"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdX69H-eTjDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns = drop_column)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBuua2IyO3up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "67acd85d-0302-476d-b5e4-2410249e653b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f15</th>\n",
              "      <th>f18</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f36</th>\n",
              "      <th>f42</th>\n",
              "      <th>f46</th>\n",
              "      <th>f48</th>\n",
              "      <th>f49</th>\n",
              "      <th>f50</th>\n",
              "      <th>f55</th>\n",
              "      <th>f56</th>\n",
              "      <th>f61</th>\n",
              "      <th>f65</th>\n",
              "      <th>f68</th>\n",
              "      <th>f71</th>\n",
              "      <th>f73</th>\n",
              "      <th>f74</th>\n",
              "      <th>f76</th>\n",
              "      <th>f77</th>\n",
              "      <th>f80</th>\n",
              "      <th>f81</th>\n",
              "      <th>f82</th>\n",
              "      <th>...</th>\n",
              "      <th>f952</th>\n",
              "      <th>f958</th>\n",
              "      <th>f959</th>\n",
              "      <th>f960</th>\n",
              "      <th>f961</th>\n",
              "      <th>f962</th>\n",
              "      <th>f963</th>\n",
              "      <th>f964</th>\n",
              "      <th>f965</th>\n",
              "      <th>f966</th>\n",
              "      <th>f967</th>\n",
              "      <th>f968</th>\n",
              "      <th>f969</th>\n",
              "      <th>f971</th>\n",
              "      <th>f973</th>\n",
              "      <th>f976</th>\n",
              "      <th>f977</th>\n",
              "      <th>f978</th>\n",
              "      <th>f979</th>\n",
              "      <th>f985</th>\n",
              "      <th>f986</th>\n",
              "      <th>f989</th>\n",
              "      <th>f990</th>\n",
              "      <th>f991</th>\n",
              "      <th>f992</th>\n",
              "      <th>f993</th>\n",
              "      <th>f994</th>\n",
              "      <th>f995</th>\n",
              "      <th>f1001</th>\n",
              "      <th>f1006</th>\n",
              "      <th>f1010</th>\n",
              "      <th>f1012</th>\n",
              "      <th>f1013</th>\n",
              "      <th>f1015</th>\n",
              "      <th>f1016</th>\n",
              "      <th>f1019</th>\n",
              "      <th>f1020</th>\n",
              "      <th>f1022</th>\n",
              "      <th>f1023</th>\n",
              "      <th>f1024</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>File Name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SK777A.jpg</th>\n",
              "      <td>0.054964736104011536</td>\n",
              "      <td>0.22152693569660187</td>\n",
              "      <td>0.20049293339252472</td>\n",
              "      <td>0.32851511240005493</td>\n",
              "      <td>0.21281714737415314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14667686820030212</td>\n",
              "      <td>0.25303351879119873</td>\n",
              "      <td>0.20519188046455383</td>\n",
              "      <td>0.17950525879859924</td>\n",
              "      <td>0.3201408386230469</td>\n",
              "      <td>0.16100464761257172</td>\n",
              "      <td>0.437704861164093</td>\n",
              "      <td>0.22956152260303497</td>\n",
              "      <td>0.22313058376312256</td>\n",
              "      <td>0.10593216121196747</td>\n",
              "      <td>0.0904526337981224</td>\n",
              "      <td>0.15168622136116028</td>\n",
              "      <td>0.16904780268669128</td>\n",
              "      <td>0.20939067006111145</td>\n",
              "      <td>0.27041947841644287</td>\n",
              "      <td>0.36809247732162476</td>\n",
              "      <td>0.3033146858215332</td>\n",
              "      <td>0.10513006895780563</td>\n",
              "      <td>0.4112304449081421</td>\n",
              "      <td>0.3631654381752014</td>\n",
              "      <td>0.03901173919439316</td>\n",
              "      <td>0.04293293133378029</td>\n",
              "      <td>0.11261092126369476</td>\n",
              "      <td>0.14219503104686737</td>\n",
              "      <td>0.23406937718391418</td>\n",
              "      <td>0.31287750601768494</td>\n",
              "      <td>0.07245940715074539</td>\n",
              "      <td>0.34887418150901794</td>\n",
              "      <td>0.04583900421857834</td>\n",
              "      <td>0.32144853472709656</td>\n",
              "      <td>0.4295426309108734</td>\n",
              "      <td>0.04616404324769974</td>\n",
              "      <td>0.38390806317329407</td>\n",
              "      <td>0.29811912775039673</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04921803995966911</td>\n",
              "      <td>0.3273155689239502</td>\n",
              "      <td>0.2614035904407501</td>\n",
              "      <td>0.23038695752620697</td>\n",
              "      <td>0.02460755966603756</td>\n",
              "      <td>0.28379911184310913</td>\n",
              "      <td>0.4427116811275482</td>\n",
              "      <td>0.07891993969678879</td>\n",
              "      <td>0.08609309792518616</td>\n",
              "      <td>0.3025645315647125</td>\n",
              "      <td>0.26012086868286133</td>\n",
              "      <td>0.14111961424350739</td>\n",
              "      <td>0.3430349826812744</td>\n",
              "      <td>0.3789758086204529</td>\n",
              "      <td>0.2318737804889679</td>\n",
              "      <td>0.2784707844257355</td>\n",
              "      <td>0.16018834710121155</td>\n",
              "      <td>0.24888819456100464</td>\n",
              "      <td>0.5535653233528137</td>\n",
              "      <td>0.04666895046830177</td>\n",
              "      <td>0.02123383991420269</td>\n",
              "      <td>0.17220571637153625</td>\n",
              "      <td>0.3136054575443268</td>\n",
              "      <td>0.2678583562374115</td>\n",
              "      <td>0.2527478337287903</td>\n",
              "      <td>0.08554912358522415</td>\n",
              "      <td>0.28667235374450684</td>\n",
              "      <td>0.38043150305747986</td>\n",
              "      <td>0.2430526465177536</td>\n",
              "      <td>0.14139991998672485</td>\n",
              "      <td>0.4723118841648102</td>\n",
              "      <td>0.39390692114830017</td>\n",
              "      <td>0.24714842438697815</td>\n",
              "      <td>0.0793328732252121</td>\n",
              "      <td>0.37838050723075867</td>\n",
              "      <td>0.16540636122226715</td>\n",
              "      <td>0.23263463377952576</td>\n",
              "      <td>0.06758081912994385</td>\n",
              "      <td>0.3017416000366211</td>\n",
              "      <td>0.13619039952754974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74Aa.jpg</th>\n",
              "      <td>0.06117307394742966</td>\n",
              "      <td>0.2447713166475296</td>\n",
              "      <td>0.22156760096549988</td>\n",
              "      <td>0.36321821808815</td>\n",
              "      <td>0.234822615981102</td>\n",
              "      <td>0.0007261084392666817</td>\n",
              "      <td>0.16223174333572388</td>\n",
              "      <td>0.2805018723011017</td>\n",
              "      <td>0.2268441766500473</td>\n",
              "      <td>0.1980280578136444</td>\n",
              "      <td>0.3550869822502136</td>\n",
              "      <td>0.17771035432815552</td>\n",
              "      <td>0.48466986417770386</td>\n",
              "      <td>0.2544008791446686</td>\n",
              "      <td>0.24735242128372192</td>\n",
              "      <td>0.11720367521047592</td>\n",
              "      <td>0.09972742944955826</td>\n",
              "      <td>0.16760236024856567</td>\n",
              "      <td>0.1875586062669754</td>\n",
              "      <td>0.23253385722637177</td>\n",
              "      <td>0.29898691177368164</td>\n",
              "      <td>0.40726080536842346</td>\n",
              "      <td>0.3357153534889221</td>\n",
              "      <td>0.1160888820886612</td>\n",
              "      <td>0.45508795976638794</td>\n",
              "      <td>0.4019779562950134</td>\n",
              "      <td>0.04345705360174179</td>\n",
              "      <td>0.04810976982116699</td>\n",
              "      <td>0.12534472346305847</td>\n",
              "      <td>0.1577388346195221</td>\n",
              "      <td>0.2586624324321747</td>\n",
              "      <td>0.3463777005672455</td>\n",
              "      <td>0.08037487417459488</td>\n",
              "      <td>0.3863106369972229</td>\n",
              "      <td>0.05096788704395294</td>\n",
              "      <td>0.35628628730773926</td>\n",
              "      <td>0.47487673163414</td>\n",
              "      <td>0.0508415661752224</td>\n",
              "      <td>0.4245606064796448</td>\n",
              "      <td>0.3295080065727234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05504245683550835</td>\n",
              "      <td>0.36234840750694275</td>\n",
              "      <td>0.28911641240119934</td>\n",
              "      <td>0.25566017627716064</td>\n",
              "      <td>0.02772459387779236</td>\n",
              "      <td>0.3136836588382721</td>\n",
              "      <td>0.48994362354278564</td>\n",
              "      <td>0.08758337050676346</td>\n",
              "      <td>0.09578624367713928</td>\n",
              "      <td>0.3354140818119049</td>\n",
              "      <td>0.2886212468147278</td>\n",
              "      <td>0.15637604892253876</td>\n",
              "      <td>0.3794548213481903</td>\n",
              "      <td>0.4189746081829071</td>\n",
              "      <td>0.2571336627006531</td>\n",
              "      <td>0.30866560339927673</td>\n",
              "      <td>0.17818191647529602</td>\n",
              "      <td>0.27475860714912415</td>\n",
              "      <td>0.6119040250778198</td>\n",
              "      <td>0.051834724843502045</td>\n",
              "      <td>0.02349783293902874</td>\n",
              "      <td>0.19095422327518463</td>\n",
              "      <td>0.3474618196487427</td>\n",
              "      <td>0.2970238924026489</td>\n",
              "      <td>0.2798535227775574</td>\n",
              "      <td>0.09486642479896545</td>\n",
              "      <td>0.31779417395591736</td>\n",
              "      <td>0.4207954704761505</td>\n",
              "      <td>0.2695974111557007</td>\n",
              "      <td>0.1560518443584442</td>\n",
              "      <td>0.522476851940155</td>\n",
              "      <td>0.43584734201431274</td>\n",
              "      <td>0.27313971519470215</td>\n",
              "      <td>0.08736691623926163</td>\n",
              "      <td>0.41897159814834595</td>\n",
              "      <td>0.18272027373313904</td>\n",
              "      <td>0.2582162916660309</td>\n",
              "      <td>0.07462792843580246</td>\n",
              "      <td>0.3336355686187744</td>\n",
              "      <td>0.15061648190021515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK780B.jpg</th>\n",
              "      <td>0.06054367125034332</td>\n",
              "      <td>0.24241478741168976</td>\n",
              "      <td>0.21943102777004242</td>\n",
              "      <td>0.3596999943256378</td>\n",
              "      <td>0.2325916886329651</td>\n",
              "      <td>0.0006419690325856209</td>\n",
              "      <td>0.16065478324890137</td>\n",
              "      <td>0.27771711349487305</td>\n",
              "      <td>0.22464905679225922</td>\n",
              "      <td>0.19615021347999573</td>\n",
              "      <td>0.3515441417694092</td>\n",
              "      <td>0.17601671814918518</td>\n",
              "      <td>0.4799085259437561</td>\n",
              "      <td>0.2518826723098755</td>\n",
              "      <td>0.244896799325943</td>\n",
              "      <td>0.11606096476316452</td>\n",
              "      <td>0.09878714382648468</td>\n",
              "      <td>0.1659887731075287</td>\n",
              "      <td>0.18568196892738342</td>\n",
              "      <td>0.2301875799894333</td>\n",
              "      <td>0.2960907220840454</td>\n",
              "      <td>0.40328991413116455</td>\n",
              "      <td>0.33243054151535034</td>\n",
              "      <td>0.1149778664112091</td>\n",
              "      <td>0.4506416618824005</td>\n",
              "      <td>0.39804312586784363</td>\n",
              "      <td>0.04300638660788536</td>\n",
              "      <td>0.04758493974804878</td>\n",
              "      <td>0.12405376136302948</td>\n",
              "      <td>0.15616299211978912</td>\n",
              "      <td>0.2561691701412201</td>\n",
              "      <td>0.3429814279079437</td>\n",
              "      <td>0.0795724019408226</td>\n",
              "      <td>0.3825153112411499</td>\n",
              "      <td>0.050447918474674225</td>\n",
              "      <td>0.3527544140815735</td>\n",
              "      <td>0.4702807366847992</td>\n",
              "      <td>0.05036735534667969</td>\n",
              "      <td>0.420439213514328</td>\n",
              "      <td>0.32632580399513245</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05445197597146034</td>\n",
              "      <td>0.35879674553871155</td>\n",
              "      <td>0.28630685806274414</td>\n",
              "      <td>0.2530979812145233</td>\n",
              "      <td>0.02740858495235443</td>\n",
              "      <td>0.310653954744339</td>\n",
              "      <td>0.48515522480010986</td>\n",
              "      <td>0.08670506626367569</td>\n",
              "      <td>0.09480354189872742</td>\n",
              "      <td>0.3320837914943695</td>\n",
              "      <td>0.28573185205459595</td>\n",
              "      <td>0.15482933819293976</td>\n",
              "      <td>0.37576255202293396</td>\n",
              "      <td>0.41491949558258057</td>\n",
              "      <td>0.2545727789402008</td>\n",
              "      <td>0.3056044578552246</td>\n",
              "      <td>0.1763577163219452</td>\n",
              "      <td>0.27213582396507263</td>\n",
              "      <td>0.6059896349906921</td>\n",
              "      <td>0.05131101608276367</td>\n",
              "      <td>0.023268306627869606</td>\n",
              "      <td>0.1890534907579422</td>\n",
              "      <td>0.3440294563770294</td>\n",
              "      <td>0.2940670847892761</td>\n",
              "      <td>0.27710554003715515</td>\n",
              "      <td>0.09392182528972626</td>\n",
              "      <td>0.31463903188705444</td>\n",
              "      <td>0.41670334339141846</td>\n",
              "      <td>0.2669062614440918</td>\n",
              "      <td>0.1545664221048355</td>\n",
              "      <td>0.5173910856246948</td>\n",
              "      <td>0.4315953850746155</td>\n",
              "      <td>0.2705046832561493</td>\n",
              "      <td>0.08655241876840591</td>\n",
              "      <td>0.4148564338684082</td>\n",
              "      <td>0.18096497654914856</td>\n",
              "      <td>0.2556228041648865</td>\n",
              "      <td>0.07391348481178284</td>\n",
              "      <td>0.330402135848999</td>\n",
              "      <td>0.1491539478302002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK751A.jpg</th>\n",
              "      <td>0.05900680646300316</td>\n",
              "      <td>0.2366606891155243</td>\n",
              "      <td>0.2142140418291092</td>\n",
              "      <td>0.3511093258857727</td>\n",
              "      <td>0.2271442860364914</td>\n",
              "      <td>0.0004365192726254463</td>\n",
              "      <td>0.15680420398712158</td>\n",
              "      <td>0.2709173858165741</td>\n",
              "      <td>0.21928906440734863</td>\n",
              "      <td>0.1915649175643921</td>\n",
              "      <td>0.3428932726383209</td>\n",
              "      <td>0.17188125848770142</td>\n",
              "      <td>0.46828243136405945</td>\n",
              "      <td>0.24573373794555664</td>\n",
              "      <td>0.23890073597431183</td>\n",
              "      <td>0.11327072232961655</td>\n",
              "      <td>0.09649118781089783</td>\n",
              "      <td>0.16204875707626343</td>\n",
              "      <td>0.18109965324401855</td>\n",
              "      <td>0.22445853054523468</td>\n",
              "      <td>0.2890188992023468</td>\n",
              "      <td>0.39359384775161743</td>\n",
              "      <td>0.3244098424911499</td>\n",
              "      <td>0.11226503551006317</td>\n",
              "      <td>0.43978482484817505</td>\n",
              "      <td>0.38843515515327454</td>\n",
              "      <td>0.0419059582054615</td>\n",
              "      <td>0.04630342125892639</td>\n",
              "      <td>0.12090153992176056</td>\n",
              "      <td>0.152315154671669</td>\n",
              "      <td>0.25008121132850647</td>\n",
              "      <td>0.33468854427337646</td>\n",
              "      <td>0.07761294394731522</td>\n",
              "      <td>0.37324798107147217</td>\n",
              "      <td>0.04917827248573303</td>\n",
              "      <td>0.34413039684295654</td>\n",
              "      <td>0.45905837416648865</td>\n",
              "      <td>0.04920944571495056</td>\n",
              "      <td>0.41037577390670776</td>\n",
              "      <td>0.3185555338859558</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05301015079021454</td>\n",
              "      <td>0.35012444853782654</td>\n",
              "      <td>0.27944663166999817</td>\n",
              "      <td>0.24684162437915802</td>\n",
              "      <td>0.026636971160769463</td>\n",
              "      <td>0.303256094455719</td>\n",
              "      <td>0.4734630286693573</td>\n",
              "      <td>0.08456045389175415</td>\n",
              "      <td>0.09240402281284332</td>\n",
              "      <td>0.32395192980766296</td>\n",
              "      <td>0.2786766588687897</td>\n",
              "      <td>0.15105263888835907</td>\n",
              "      <td>0.3667469024658203</td>\n",
              "      <td>0.4050178825855255</td>\n",
              "      <td>0.24831975996494293</td>\n",
              "      <td>0.29812976717948914</td>\n",
              "      <td>0.17190344631671906</td>\n",
              "      <td>0.26573166251182556</td>\n",
              "      <td>0.5915480256080627</td>\n",
              "      <td>0.05003223940730095</td>\n",
              "      <td>0.02270786091685295</td>\n",
              "      <td>0.18441233038902283</td>\n",
              "      <td>0.3356483578681946</td>\n",
              "      <td>0.28684720396995544</td>\n",
              "      <td>0.27039557695388794</td>\n",
              "      <td>0.09161534905433655</td>\n",
              "      <td>0.3069348931312561</td>\n",
              "      <td>0.40671131014823914</td>\n",
              "      <td>0.26033517718315125</td>\n",
              "      <td>0.15093936026096344</td>\n",
              "      <td>0.5049728155136108</td>\n",
              "      <td>0.4212131202220917</td>\n",
              "      <td>0.264070600271225</td>\n",
              "      <td>0.08456360548734665</td>\n",
              "      <td>0.4048081934452057</td>\n",
              "      <td>0.17667895555496216</td>\n",
              "      <td>0.2492901235818863</td>\n",
              "      <td>0.0721689909696579</td>\n",
              "      <td>0.322506844997406</td>\n",
              "      <td>0.14558279514312744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK772A.jpg</th>\n",
              "      <td>0.056103043258190155</td>\n",
              "      <td>0.22578881680965424</td>\n",
              "      <td>0.20435699820518494</td>\n",
              "      <td>0.33487796783447266</td>\n",
              "      <td>0.2168518751859665</td>\n",
              "      <td>4.834122955799103e-05</td>\n",
              "      <td>0.1495288759469986</td>\n",
              "      <td>0.25806987285614014</td>\n",
              "      <td>0.20916184782981873</td>\n",
              "      <td>0.18290142714977264</td>\n",
              "      <td>0.3265482783317566</td>\n",
              "      <td>0.16406765580177307</td>\n",
              "      <td>0.4463159441947937</td>\n",
              "      <td>0.2341158539056778</td>\n",
              "      <td>0.22757169604301453</td>\n",
              "      <td>0.1079988032579422</td>\n",
              "      <td>0.09215317666530609</td>\n",
              "      <td>0.1546044647693634</td>\n",
              "      <td>0.1724417805671692</td>\n",
              "      <td>0.21363399922847748</td>\n",
              "      <td>0.2756573259830475</td>\n",
              "      <td>0.37527403235435486</td>\n",
              "      <td>0.3092553913593292</td>\n",
              "      <td>0.10713937878608704</td>\n",
              "      <td>0.4192717969417572</td>\n",
              "      <td>0.37028175592422485</td>\n",
              "      <td>0.03982679173350334</td>\n",
              "      <td>0.043882112950086594</td>\n",
              "      <td>0.1149456799030304</td>\n",
              "      <td>0.14504501223564148</td>\n",
              "      <td>0.23857854306697845</td>\n",
              "      <td>0.3190198242664337</td>\n",
              "      <td>0.07391072064638138</td>\n",
              "      <td>0.35573819279670715</td>\n",
              "      <td>0.046779390424489975</td>\n",
              "      <td>0.3278360664844513</td>\n",
              "      <td>0.43785467743873596</td>\n",
              "      <td>0.04702167212963104</td>\n",
              "      <td>0.3913617432117462</td>\n",
              "      <td>0.30387431383132935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050285954028367996</td>\n",
              "      <td>0.33373886346817017</td>\n",
              "      <td>0.2664847671985626</td>\n",
              "      <td>0.23502083122730255</td>\n",
              "      <td>0.025179071351885796</td>\n",
              "      <td>0.28927847743034363</td>\n",
              "      <td>0.4513716995716095</td>\n",
              "      <td>0.08050838857889175</td>\n",
              "      <td>0.08787034451961517</td>\n",
              "      <td>0.30858752131462097</td>\n",
              "      <td>0.2653464376926422</td>\n",
              "      <td>0.14391690492630005</td>\n",
              "      <td>0.349712610244751</td>\n",
              "      <td>0.3863096237182617</td>\n",
              "      <td>0.2365051954984665</td>\n",
              "      <td>0.2840070426464081</td>\n",
              "      <td>0.1634874939918518</td>\n",
              "      <td>0.2536315619945526</td>\n",
              "      <td>0.564261794090271</td>\n",
              "      <td>0.047616101801395416</td>\n",
              "      <td>0.021648945286870003</td>\n",
              "      <td>0.17564328014850616</td>\n",
              "      <td>0.319813072681427</td>\n",
              "      <td>0.27320587635040283</td>\n",
              "      <td>0.25771769881248474</td>\n",
              "      <td>0.08725745975971222</td>\n",
              "      <td>0.29237857460975647</td>\n",
              "      <td>0.38783228397369385</td>\n",
              "      <td>0.24791964888572693</td>\n",
              "      <td>0.14408636093139648</td>\n",
              "      <td>0.4815097153186798</td>\n",
              "      <td>0.4015967547893524</td>\n",
              "      <td>0.2519139349460602</td>\n",
              "      <td>0.08080592006444931</td>\n",
              "      <td>0.38582292199134827</td>\n",
              "      <td>0.16858088970184326</td>\n",
              "      <td>0.237325057387352</td>\n",
              "      <td>0.06887291371822357</td>\n",
              "      <td>0.3075893521308899</td>\n",
              "      <td>0.13883543014526367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 493 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              f1  ...                f1024\n",
              "File Name                         ...                     \n",
              "SK777A.jpg  0.054964736104011536  ...  0.13619039952754974\n",
              "74Aa.jpg     0.06117307394742966  ...  0.15061648190021515\n",
              "SK780B.jpg   0.06054367125034332  ...   0.1491539478302002\n",
              "SK751A.jpg   0.05900680646300316  ...  0.14558279514312744\n",
              "SK772A.jpg  0.056103043258190155  ...  0.13883543014526367\n",
              "\n",
              "[5 rows x 493 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEGnDV4U-P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_name = []\n",
        "for i in range(1,494):\n",
        "  column_name.append('f' + str(i))\n",
        "\n",
        "df.columns = column_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtvJHSApVdBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "e2f7bb1d-1c6e-4006-c65e-37461d1bee00"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f35</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>f39</th>\n",
              "      <th>f40</th>\n",
              "      <th>...</th>\n",
              "      <th>f454</th>\n",
              "      <th>f455</th>\n",
              "      <th>f456</th>\n",
              "      <th>f457</th>\n",
              "      <th>f458</th>\n",
              "      <th>f459</th>\n",
              "      <th>f460</th>\n",
              "      <th>f461</th>\n",
              "      <th>f462</th>\n",
              "      <th>f463</th>\n",
              "      <th>f464</th>\n",
              "      <th>f465</th>\n",
              "      <th>f466</th>\n",
              "      <th>f467</th>\n",
              "      <th>f468</th>\n",
              "      <th>f469</th>\n",
              "      <th>f470</th>\n",
              "      <th>f471</th>\n",
              "      <th>f472</th>\n",
              "      <th>f473</th>\n",
              "      <th>f474</th>\n",
              "      <th>f475</th>\n",
              "      <th>f476</th>\n",
              "      <th>f477</th>\n",
              "      <th>f478</th>\n",
              "      <th>f479</th>\n",
              "      <th>f480</th>\n",
              "      <th>f481</th>\n",
              "      <th>f482</th>\n",
              "      <th>f483</th>\n",
              "      <th>f484</th>\n",
              "      <th>f485</th>\n",
              "      <th>f486</th>\n",
              "      <th>f487</th>\n",
              "      <th>f488</th>\n",
              "      <th>f489</th>\n",
              "      <th>f490</th>\n",
              "      <th>f491</th>\n",
              "      <th>f492</th>\n",
              "      <th>f493</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>File Name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SK777A.jpg</th>\n",
              "      <td>0.054964736104011536</td>\n",
              "      <td>0.22152693569660187</td>\n",
              "      <td>0.20049293339252472</td>\n",
              "      <td>0.32851511240005493</td>\n",
              "      <td>0.21281714737415314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14667686820030212</td>\n",
              "      <td>0.25303351879119873</td>\n",
              "      <td>0.20519188046455383</td>\n",
              "      <td>0.17950525879859924</td>\n",
              "      <td>0.3201408386230469</td>\n",
              "      <td>0.16100464761257172</td>\n",
              "      <td>0.437704861164093</td>\n",
              "      <td>0.22956152260303497</td>\n",
              "      <td>0.22313058376312256</td>\n",
              "      <td>0.10593216121196747</td>\n",
              "      <td>0.0904526337981224</td>\n",
              "      <td>0.15168622136116028</td>\n",
              "      <td>0.16904780268669128</td>\n",
              "      <td>0.20939067006111145</td>\n",
              "      <td>0.27041947841644287</td>\n",
              "      <td>0.36809247732162476</td>\n",
              "      <td>0.3033146858215332</td>\n",
              "      <td>0.10513006895780563</td>\n",
              "      <td>0.4112304449081421</td>\n",
              "      <td>0.3631654381752014</td>\n",
              "      <td>0.03901173919439316</td>\n",
              "      <td>0.04293293133378029</td>\n",
              "      <td>0.11261092126369476</td>\n",
              "      <td>0.14219503104686737</td>\n",
              "      <td>0.23406937718391418</td>\n",
              "      <td>0.31287750601768494</td>\n",
              "      <td>0.07245940715074539</td>\n",
              "      <td>0.34887418150901794</td>\n",
              "      <td>0.04583900421857834</td>\n",
              "      <td>0.32144853472709656</td>\n",
              "      <td>0.4295426309108734</td>\n",
              "      <td>0.04616404324769974</td>\n",
              "      <td>0.38390806317329407</td>\n",
              "      <td>0.29811912775039673</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04921803995966911</td>\n",
              "      <td>0.3273155689239502</td>\n",
              "      <td>0.2614035904407501</td>\n",
              "      <td>0.23038695752620697</td>\n",
              "      <td>0.02460755966603756</td>\n",
              "      <td>0.28379911184310913</td>\n",
              "      <td>0.4427116811275482</td>\n",
              "      <td>0.07891993969678879</td>\n",
              "      <td>0.08609309792518616</td>\n",
              "      <td>0.3025645315647125</td>\n",
              "      <td>0.26012086868286133</td>\n",
              "      <td>0.14111961424350739</td>\n",
              "      <td>0.3430349826812744</td>\n",
              "      <td>0.3789758086204529</td>\n",
              "      <td>0.2318737804889679</td>\n",
              "      <td>0.2784707844257355</td>\n",
              "      <td>0.16018834710121155</td>\n",
              "      <td>0.24888819456100464</td>\n",
              "      <td>0.5535653233528137</td>\n",
              "      <td>0.04666895046830177</td>\n",
              "      <td>0.02123383991420269</td>\n",
              "      <td>0.17220571637153625</td>\n",
              "      <td>0.3136054575443268</td>\n",
              "      <td>0.2678583562374115</td>\n",
              "      <td>0.2527478337287903</td>\n",
              "      <td>0.08554912358522415</td>\n",
              "      <td>0.28667235374450684</td>\n",
              "      <td>0.38043150305747986</td>\n",
              "      <td>0.2430526465177536</td>\n",
              "      <td>0.14139991998672485</td>\n",
              "      <td>0.4723118841648102</td>\n",
              "      <td>0.39390692114830017</td>\n",
              "      <td>0.24714842438697815</td>\n",
              "      <td>0.0793328732252121</td>\n",
              "      <td>0.37838050723075867</td>\n",
              "      <td>0.16540636122226715</td>\n",
              "      <td>0.23263463377952576</td>\n",
              "      <td>0.06758081912994385</td>\n",
              "      <td>0.3017416000366211</td>\n",
              "      <td>0.13619039952754974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74Aa.jpg</th>\n",
              "      <td>0.06117307394742966</td>\n",
              "      <td>0.2447713166475296</td>\n",
              "      <td>0.22156760096549988</td>\n",
              "      <td>0.36321821808815</td>\n",
              "      <td>0.234822615981102</td>\n",
              "      <td>0.0007261084392666817</td>\n",
              "      <td>0.16223174333572388</td>\n",
              "      <td>0.2805018723011017</td>\n",
              "      <td>0.2268441766500473</td>\n",
              "      <td>0.1980280578136444</td>\n",
              "      <td>0.3550869822502136</td>\n",
              "      <td>0.17771035432815552</td>\n",
              "      <td>0.48466986417770386</td>\n",
              "      <td>0.2544008791446686</td>\n",
              "      <td>0.24735242128372192</td>\n",
              "      <td>0.11720367521047592</td>\n",
              "      <td>0.09972742944955826</td>\n",
              "      <td>0.16760236024856567</td>\n",
              "      <td>0.1875586062669754</td>\n",
              "      <td>0.23253385722637177</td>\n",
              "      <td>0.29898691177368164</td>\n",
              "      <td>0.40726080536842346</td>\n",
              "      <td>0.3357153534889221</td>\n",
              "      <td>0.1160888820886612</td>\n",
              "      <td>0.45508795976638794</td>\n",
              "      <td>0.4019779562950134</td>\n",
              "      <td>0.04345705360174179</td>\n",
              "      <td>0.04810976982116699</td>\n",
              "      <td>0.12534472346305847</td>\n",
              "      <td>0.1577388346195221</td>\n",
              "      <td>0.2586624324321747</td>\n",
              "      <td>0.3463777005672455</td>\n",
              "      <td>0.08037487417459488</td>\n",
              "      <td>0.3863106369972229</td>\n",
              "      <td>0.05096788704395294</td>\n",
              "      <td>0.35628628730773926</td>\n",
              "      <td>0.47487673163414</td>\n",
              "      <td>0.0508415661752224</td>\n",
              "      <td>0.4245606064796448</td>\n",
              "      <td>0.3295080065727234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05504245683550835</td>\n",
              "      <td>0.36234840750694275</td>\n",
              "      <td>0.28911641240119934</td>\n",
              "      <td>0.25566017627716064</td>\n",
              "      <td>0.02772459387779236</td>\n",
              "      <td>0.3136836588382721</td>\n",
              "      <td>0.48994362354278564</td>\n",
              "      <td>0.08758337050676346</td>\n",
              "      <td>0.09578624367713928</td>\n",
              "      <td>0.3354140818119049</td>\n",
              "      <td>0.2886212468147278</td>\n",
              "      <td>0.15637604892253876</td>\n",
              "      <td>0.3794548213481903</td>\n",
              "      <td>0.4189746081829071</td>\n",
              "      <td>0.2571336627006531</td>\n",
              "      <td>0.30866560339927673</td>\n",
              "      <td>0.17818191647529602</td>\n",
              "      <td>0.27475860714912415</td>\n",
              "      <td>0.6119040250778198</td>\n",
              "      <td>0.051834724843502045</td>\n",
              "      <td>0.02349783293902874</td>\n",
              "      <td>0.19095422327518463</td>\n",
              "      <td>0.3474618196487427</td>\n",
              "      <td>0.2970238924026489</td>\n",
              "      <td>0.2798535227775574</td>\n",
              "      <td>0.09486642479896545</td>\n",
              "      <td>0.31779417395591736</td>\n",
              "      <td>0.4207954704761505</td>\n",
              "      <td>0.2695974111557007</td>\n",
              "      <td>0.1560518443584442</td>\n",
              "      <td>0.522476851940155</td>\n",
              "      <td>0.43584734201431274</td>\n",
              "      <td>0.27313971519470215</td>\n",
              "      <td>0.08736691623926163</td>\n",
              "      <td>0.41897159814834595</td>\n",
              "      <td>0.18272027373313904</td>\n",
              "      <td>0.2582162916660309</td>\n",
              "      <td>0.07462792843580246</td>\n",
              "      <td>0.3336355686187744</td>\n",
              "      <td>0.15061648190021515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK780B.jpg</th>\n",
              "      <td>0.06054367125034332</td>\n",
              "      <td>0.24241478741168976</td>\n",
              "      <td>0.21943102777004242</td>\n",
              "      <td>0.3596999943256378</td>\n",
              "      <td>0.2325916886329651</td>\n",
              "      <td>0.0006419690325856209</td>\n",
              "      <td>0.16065478324890137</td>\n",
              "      <td>0.27771711349487305</td>\n",
              "      <td>0.22464905679225922</td>\n",
              "      <td>0.19615021347999573</td>\n",
              "      <td>0.3515441417694092</td>\n",
              "      <td>0.17601671814918518</td>\n",
              "      <td>0.4799085259437561</td>\n",
              "      <td>0.2518826723098755</td>\n",
              "      <td>0.244896799325943</td>\n",
              "      <td>0.11606096476316452</td>\n",
              "      <td>0.09878714382648468</td>\n",
              "      <td>0.1659887731075287</td>\n",
              "      <td>0.18568196892738342</td>\n",
              "      <td>0.2301875799894333</td>\n",
              "      <td>0.2960907220840454</td>\n",
              "      <td>0.40328991413116455</td>\n",
              "      <td>0.33243054151535034</td>\n",
              "      <td>0.1149778664112091</td>\n",
              "      <td>0.4506416618824005</td>\n",
              "      <td>0.39804312586784363</td>\n",
              "      <td>0.04300638660788536</td>\n",
              "      <td>0.04758493974804878</td>\n",
              "      <td>0.12405376136302948</td>\n",
              "      <td>0.15616299211978912</td>\n",
              "      <td>0.2561691701412201</td>\n",
              "      <td>0.3429814279079437</td>\n",
              "      <td>0.0795724019408226</td>\n",
              "      <td>0.3825153112411499</td>\n",
              "      <td>0.050447918474674225</td>\n",
              "      <td>0.3527544140815735</td>\n",
              "      <td>0.4702807366847992</td>\n",
              "      <td>0.05036735534667969</td>\n",
              "      <td>0.420439213514328</td>\n",
              "      <td>0.32632580399513245</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05445197597146034</td>\n",
              "      <td>0.35879674553871155</td>\n",
              "      <td>0.28630685806274414</td>\n",
              "      <td>0.2530979812145233</td>\n",
              "      <td>0.02740858495235443</td>\n",
              "      <td>0.310653954744339</td>\n",
              "      <td>0.48515522480010986</td>\n",
              "      <td>0.08670506626367569</td>\n",
              "      <td>0.09480354189872742</td>\n",
              "      <td>0.3320837914943695</td>\n",
              "      <td>0.28573185205459595</td>\n",
              "      <td>0.15482933819293976</td>\n",
              "      <td>0.37576255202293396</td>\n",
              "      <td>0.41491949558258057</td>\n",
              "      <td>0.2545727789402008</td>\n",
              "      <td>0.3056044578552246</td>\n",
              "      <td>0.1763577163219452</td>\n",
              "      <td>0.27213582396507263</td>\n",
              "      <td>0.6059896349906921</td>\n",
              "      <td>0.05131101608276367</td>\n",
              "      <td>0.023268306627869606</td>\n",
              "      <td>0.1890534907579422</td>\n",
              "      <td>0.3440294563770294</td>\n",
              "      <td>0.2940670847892761</td>\n",
              "      <td>0.27710554003715515</td>\n",
              "      <td>0.09392182528972626</td>\n",
              "      <td>0.31463903188705444</td>\n",
              "      <td>0.41670334339141846</td>\n",
              "      <td>0.2669062614440918</td>\n",
              "      <td>0.1545664221048355</td>\n",
              "      <td>0.5173910856246948</td>\n",
              "      <td>0.4315953850746155</td>\n",
              "      <td>0.2705046832561493</td>\n",
              "      <td>0.08655241876840591</td>\n",
              "      <td>0.4148564338684082</td>\n",
              "      <td>0.18096497654914856</td>\n",
              "      <td>0.2556228041648865</td>\n",
              "      <td>0.07391348481178284</td>\n",
              "      <td>0.330402135848999</td>\n",
              "      <td>0.1491539478302002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK751A.jpg</th>\n",
              "      <td>0.05900680646300316</td>\n",
              "      <td>0.2366606891155243</td>\n",
              "      <td>0.2142140418291092</td>\n",
              "      <td>0.3511093258857727</td>\n",
              "      <td>0.2271442860364914</td>\n",
              "      <td>0.0004365192726254463</td>\n",
              "      <td>0.15680420398712158</td>\n",
              "      <td>0.2709173858165741</td>\n",
              "      <td>0.21928906440734863</td>\n",
              "      <td>0.1915649175643921</td>\n",
              "      <td>0.3428932726383209</td>\n",
              "      <td>0.17188125848770142</td>\n",
              "      <td>0.46828243136405945</td>\n",
              "      <td>0.24573373794555664</td>\n",
              "      <td>0.23890073597431183</td>\n",
              "      <td>0.11327072232961655</td>\n",
              "      <td>0.09649118781089783</td>\n",
              "      <td>0.16204875707626343</td>\n",
              "      <td>0.18109965324401855</td>\n",
              "      <td>0.22445853054523468</td>\n",
              "      <td>0.2890188992023468</td>\n",
              "      <td>0.39359384775161743</td>\n",
              "      <td>0.3244098424911499</td>\n",
              "      <td>0.11226503551006317</td>\n",
              "      <td>0.43978482484817505</td>\n",
              "      <td>0.38843515515327454</td>\n",
              "      <td>0.0419059582054615</td>\n",
              "      <td>0.04630342125892639</td>\n",
              "      <td>0.12090153992176056</td>\n",
              "      <td>0.152315154671669</td>\n",
              "      <td>0.25008121132850647</td>\n",
              "      <td>0.33468854427337646</td>\n",
              "      <td>0.07761294394731522</td>\n",
              "      <td>0.37324798107147217</td>\n",
              "      <td>0.04917827248573303</td>\n",
              "      <td>0.34413039684295654</td>\n",
              "      <td>0.45905837416648865</td>\n",
              "      <td>0.04920944571495056</td>\n",
              "      <td>0.41037577390670776</td>\n",
              "      <td>0.3185555338859558</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05301015079021454</td>\n",
              "      <td>0.35012444853782654</td>\n",
              "      <td>0.27944663166999817</td>\n",
              "      <td>0.24684162437915802</td>\n",
              "      <td>0.026636971160769463</td>\n",
              "      <td>0.303256094455719</td>\n",
              "      <td>0.4734630286693573</td>\n",
              "      <td>0.08456045389175415</td>\n",
              "      <td>0.09240402281284332</td>\n",
              "      <td>0.32395192980766296</td>\n",
              "      <td>0.2786766588687897</td>\n",
              "      <td>0.15105263888835907</td>\n",
              "      <td>0.3667469024658203</td>\n",
              "      <td>0.4050178825855255</td>\n",
              "      <td>0.24831975996494293</td>\n",
              "      <td>0.29812976717948914</td>\n",
              "      <td>0.17190344631671906</td>\n",
              "      <td>0.26573166251182556</td>\n",
              "      <td>0.5915480256080627</td>\n",
              "      <td>0.05003223940730095</td>\n",
              "      <td>0.02270786091685295</td>\n",
              "      <td>0.18441233038902283</td>\n",
              "      <td>0.3356483578681946</td>\n",
              "      <td>0.28684720396995544</td>\n",
              "      <td>0.27039557695388794</td>\n",
              "      <td>0.09161534905433655</td>\n",
              "      <td>0.3069348931312561</td>\n",
              "      <td>0.40671131014823914</td>\n",
              "      <td>0.26033517718315125</td>\n",
              "      <td>0.15093936026096344</td>\n",
              "      <td>0.5049728155136108</td>\n",
              "      <td>0.4212131202220917</td>\n",
              "      <td>0.264070600271225</td>\n",
              "      <td>0.08456360548734665</td>\n",
              "      <td>0.4048081934452057</td>\n",
              "      <td>0.17667895555496216</td>\n",
              "      <td>0.2492901235818863</td>\n",
              "      <td>0.0721689909696579</td>\n",
              "      <td>0.322506844997406</td>\n",
              "      <td>0.14558279514312744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK772A.jpg</th>\n",
              "      <td>0.056103043258190155</td>\n",
              "      <td>0.22578881680965424</td>\n",
              "      <td>0.20435699820518494</td>\n",
              "      <td>0.33487796783447266</td>\n",
              "      <td>0.2168518751859665</td>\n",
              "      <td>4.834122955799103e-05</td>\n",
              "      <td>0.1495288759469986</td>\n",
              "      <td>0.25806987285614014</td>\n",
              "      <td>0.20916184782981873</td>\n",
              "      <td>0.18290142714977264</td>\n",
              "      <td>0.3265482783317566</td>\n",
              "      <td>0.16406765580177307</td>\n",
              "      <td>0.4463159441947937</td>\n",
              "      <td>0.2341158539056778</td>\n",
              "      <td>0.22757169604301453</td>\n",
              "      <td>0.1079988032579422</td>\n",
              "      <td>0.09215317666530609</td>\n",
              "      <td>0.1546044647693634</td>\n",
              "      <td>0.1724417805671692</td>\n",
              "      <td>0.21363399922847748</td>\n",
              "      <td>0.2756573259830475</td>\n",
              "      <td>0.37527403235435486</td>\n",
              "      <td>0.3092553913593292</td>\n",
              "      <td>0.10713937878608704</td>\n",
              "      <td>0.4192717969417572</td>\n",
              "      <td>0.37028175592422485</td>\n",
              "      <td>0.03982679173350334</td>\n",
              "      <td>0.043882112950086594</td>\n",
              "      <td>0.1149456799030304</td>\n",
              "      <td>0.14504501223564148</td>\n",
              "      <td>0.23857854306697845</td>\n",
              "      <td>0.3190198242664337</td>\n",
              "      <td>0.07391072064638138</td>\n",
              "      <td>0.35573819279670715</td>\n",
              "      <td>0.046779390424489975</td>\n",
              "      <td>0.3278360664844513</td>\n",
              "      <td>0.43785467743873596</td>\n",
              "      <td>0.04702167212963104</td>\n",
              "      <td>0.3913617432117462</td>\n",
              "      <td>0.30387431383132935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050285954028367996</td>\n",
              "      <td>0.33373886346817017</td>\n",
              "      <td>0.2664847671985626</td>\n",
              "      <td>0.23502083122730255</td>\n",
              "      <td>0.025179071351885796</td>\n",
              "      <td>0.28927847743034363</td>\n",
              "      <td>0.4513716995716095</td>\n",
              "      <td>0.08050838857889175</td>\n",
              "      <td>0.08787034451961517</td>\n",
              "      <td>0.30858752131462097</td>\n",
              "      <td>0.2653464376926422</td>\n",
              "      <td>0.14391690492630005</td>\n",
              "      <td>0.349712610244751</td>\n",
              "      <td>0.3863096237182617</td>\n",
              "      <td>0.2365051954984665</td>\n",
              "      <td>0.2840070426464081</td>\n",
              "      <td>0.1634874939918518</td>\n",
              "      <td>0.2536315619945526</td>\n",
              "      <td>0.564261794090271</td>\n",
              "      <td>0.047616101801395416</td>\n",
              "      <td>0.021648945286870003</td>\n",
              "      <td>0.17564328014850616</td>\n",
              "      <td>0.319813072681427</td>\n",
              "      <td>0.27320587635040283</td>\n",
              "      <td>0.25771769881248474</td>\n",
              "      <td>0.08725745975971222</td>\n",
              "      <td>0.29237857460975647</td>\n",
              "      <td>0.38783228397369385</td>\n",
              "      <td>0.24791964888572693</td>\n",
              "      <td>0.14408636093139648</td>\n",
              "      <td>0.4815097153186798</td>\n",
              "      <td>0.4015967547893524</td>\n",
              "      <td>0.2519139349460602</td>\n",
              "      <td>0.08080592006444931</td>\n",
              "      <td>0.38582292199134827</td>\n",
              "      <td>0.16858088970184326</td>\n",
              "      <td>0.237325057387352</td>\n",
              "      <td>0.06887291371822357</td>\n",
              "      <td>0.3075893521308899</td>\n",
              "      <td>0.13883543014526367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 493 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              f1  ...                 f493\n",
              "File Name                         ...                     \n",
              "SK777A.jpg  0.054964736104011536  ...  0.13619039952754974\n",
              "74Aa.jpg     0.06117307394742966  ...  0.15061648190021515\n",
              "SK780B.jpg   0.06054367125034332  ...   0.1491539478302002\n",
              "SK751A.jpg   0.05900680646300316  ...  0.14558279514312744\n",
              "SK772A.jpg  0.056103043258190155  ...  0.13883543014526367\n",
              "\n",
              "[5 rows x 493 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyRJWzqbO3rU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " df.to_excel('output.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}