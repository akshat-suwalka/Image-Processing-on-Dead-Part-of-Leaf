{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cocoon Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djj6O2U81ZHH",
        "colab_type": "text"
      },
      "source": [
        "# For vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq78J_ZIn38P",
        "colab_type": "code",
        "outputId": "9b6d459e-fad6-4a4d-c3f8-118f8fde7636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# Load the VGG model\n",
        "pre_trained_model = VGG16(input_shape = (224, 224, 3), \n",
        "                                include_top = False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmzA4Js_o4-0",
        "colab_type": "code",
        "outputId": "90dcab64-bbe7-408b-c638-e8b848781bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Freeze the layers except the last 1 layers\n",
        "#for layer in pre_trained_model.layers[:-4]:\n",
        "#  layer.trainable = False\n",
        "  \n",
        "# Check the trainable status of the individual layers\n",
        "for layer in pre_trained_model.layers:\n",
        "  print(layer,layer.trainable)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7fa2492b2710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20ea415f8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20ea41630> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1ea518> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1f8f60> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1fbe80> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e203c50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e190b00> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e196358> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e19b1d0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1a0b70> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1aea20> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1b42b0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1b95c0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1c0a20> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e1cc940> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e152208> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa20e157588> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa20e1607f0> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxMLsbQCp0yg",
        "colab_type": "code",
        "outputId": "1747f773-ae8a-403d-dc0c-e24582701427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        " \n",
        "# Create the model\n",
        "model = models.Sequential()\n",
        " \n",
        "# Add the vgg convolutional base model\n",
        "model.add(pre_trained_model)\n",
        " \n",
        "# Add new layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        " \n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              25691136  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 40,406,849\n",
            "Trainable params: 40,406,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvlmlxwDtD4C",
        "colab_type": "text"
      },
      "source": [
        "# Dataset from Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1usVCS6mlzM3",
        "colab_type": "code",
        "outputId": "8c6834be-07f8-4617-a3e8-4cf795019afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtPEZEhEtEDV",
        "colab_type": "code",
        "outputId": "a178add4-eb0c-4543-8bff-99dd53082d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/Cocoon Dataset.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/content/Cocoon Dataset'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'Train' )\n",
        "validation_dir = os.path.join( base_dir, 'Test')\n",
        "\n",
        "\n",
        "train_Male_dir = os.path.join(train_dir, 'Male') # Directory with our training cat pictures\n",
        "train_Female_dir = os.path.join(train_dir, 'Female') # Directory with our training dog pictures\n",
        "validation_Male_dir = os.path.join(validation_dir, 'Male') # Directory with our validation cat pictures\n",
        "validation_Female_dir = os.path.join(validation_dir, 'Female')# Directory with our validation dog pictures\n",
        "\n",
        "train_Male_fnames = os.listdir(train_Male_dir)\n",
        "train_Female_fnames = os.listdir(train_Female_dir)\n",
        "validation_Male_fnames = os.listdir(validation_Male_dir)\n",
        "validation_Female_fnames = os.listdir(validation_Female_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 21,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (224, 224))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 10,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (224, 224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 357 images belonging to 2 classes.\n",
            "Found 90 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5eN2wkrtHPr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llDiaK5rtHf0",
        "colab_type": "code",
        "outputId": "8efcf803-c6c1-4299-f549-89b1dccba2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer = optimizers.adam(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch = train_generator.samples/train_generator.batch_size ,\n",
        "            epochs = 30,\n",
        "            validation_data = validation_generator,\n",
        "            validation_steps = validation_generator.samples/validation_generator.batch_size,\n",
        "            verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "17/17 [==============================] - 109s 6s/step - loss: 0.7832 - acc: 0.4594 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 2/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6884 - acc: 0.5686 - val_loss: 0.6869 - val_acc: 0.5556\n",
            "Epoch 3/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6895 - acc: 0.5714 - val_loss: 0.6928 - val_acc: 0.5556\n",
            "Epoch 4/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6978 - acc: 0.5462 - val_loss: 0.6880 - val_acc: 0.5556\n",
            "Epoch 5/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6910 - acc: 0.5602 - val_loss: 0.6880 - val_acc: 0.5556\n",
            "Epoch 6/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6892 - acc: 0.5686 - val_loss: 0.6876 - val_acc: 0.5556\n",
            "Epoch 7/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6867 - acc: 0.5714 - val_loss: 0.6882 - val_acc: 0.5556\n",
            "Epoch 8/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6866 - acc: 0.5714 - val_loss: 0.6871 - val_acc: 0.5556\n",
            "Epoch 9/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6862 - acc: 0.5630 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 10/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6837 - acc: 0.5686 - val_loss: 0.6891 - val_acc: 0.5556\n",
            "Epoch 11/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6868 - acc: 0.5686 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 12/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6858 - acc: 0.5686 - val_loss: 0.6883 - val_acc: 0.5556\n",
            "Epoch 13/30\n",
            "17/17 [==============================] - 103s 6s/step - loss: 0.6885 - acc: 0.5686 - val_loss: 0.6873 - val_acc: 0.5556\n",
            "Epoch 14/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6846 - acc: 0.5658 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 15/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6849 - acc: 0.5686 - val_loss: 0.6869 - val_acc: 0.5556\n",
            "Epoch 16/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6869 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 17/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6842 - acc: 0.5686 - val_loss: 0.6891 - val_acc: 0.5556\n",
            "Epoch 18/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6869 - acc: 0.5686 - val_loss: 0.6871 - val_acc: 0.5556\n",
            "Epoch 19/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6826 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 20/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6845 - acc: 0.5686 - val_loss: 0.6882 - val_acc: 0.5556\n",
            "Epoch 21/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6816 - acc: 0.5686 - val_loss: 0.6883 - val_acc: 0.5556\n",
            "Epoch 22/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6791 - acc: 0.5686 - val_loss: 0.6909 - val_acc: 0.5556\n",
            "Epoch 23/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.7002 - acc: 0.5602 - val_loss: 0.6875 - val_acc: 0.5556\n",
            "Epoch 24/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6897 - acc: 0.5686 - val_loss: 0.6872 - val_acc: 0.5556\n",
            "Epoch 25/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6926 - acc: 0.5658 - val_loss: 0.6907 - val_acc: 0.5556\n",
            "Epoch 26/30\n",
            "17/17 [==============================] - 102s 6s/step - loss: 0.6883 - acc: 0.5686 - val_loss: 0.6872 - val_acc: 0.5556\n",
            "Epoch 27/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6874 - acc: 0.5686 - val_loss: 0.6870 - val_acc: 0.5556\n",
            "Epoch 28/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6859 - acc: 0.5686 - val_loss: 0.6874 - val_acc: 0.5556\n",
            "Epoch 29/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6870 - acc: 0.5686 - val_loss: 0.6871 - val_acc: 0.5556\n",
            "Epoch 30/30\n",
            "17/17 [==============================] - 101s 6s/step - loss: 0.6865 - acc: 0.5686 - val_loss: 0.6873 - val_acc: 0.5556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0HrEb4itKxY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atWMdc4QtK7f",
        "colab_type": "code",
        "outputId": "b8f8451a-1ccf-4796-d7d0-341b1b528073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzeW2NZEd7MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSAEkd3qA_m3",
        "colab_type": "code",
        "outputId": "c5dbbbff-fe72-4f02-a79f-c6f13e5f4082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "validation_male_list = []\n",
        "for name in validation_Male_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Test/Male/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  validation_male_list.append(intermediate_output_list)\n",
        "  \n",
        "validation_male_list = np.array(validation_male_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJPPvsvpLdFp",
        "colab_type": "code",
        "outputId": "a52ac649-ae97-4dd3-8484-c29d9c706dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "validation_female_list = []\n",
        "for name in validation_Female_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Test/Female/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  validation_female_list.append(intermediate_output_list)\n",
        "  \n",
        "validation_female_list = np.array(validation_female_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFstj1gWL6QO",
        "colab_type": "code",
        "outputId": "7367160c-24c1-41d6-c07f-c3cb95aa1627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train_male_list = []\n",
        "for name in train_Male_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Train/Male/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  train_male_list.append(intermediate_output_list)\n",
        "  \n",
        "train_male_list = np.array(train_male_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UoTR8WjMKJh",
        "colab_type": "code",
        "outputId": "0acc8ab6-33c7-41a6-dd7e-c04a13bf1596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train_female_list = []\n",
        "for name in train_Female_fnames:\n",
        "  base_dir = '/content/Cocoon Dataset/Train/Female/' + name\n",
        "  img = cv2.imread(base_dir)\n",
        "  img = cv2.resize(img,(224,224))\n",
        "  img = np.reshape(img,[1,224,224,3])\n",
        "  matrix_test = img\n",
        "  layer_name = 'dense_1'\n",
        "  intermediate_layer_model = Model(input = model.input, output = model.get_layer(layer_name).output)\n",
        "  intermediate_output = intermediate_layer_model.predict(matrix_test)\n",
        "  intermediate_output_list = [name] + intermediate_output[0].tolist()\n",
        "  train_female_list.append(intermediate_output_list)\n",
        "  \n",
        "train_female_list = np.array(train_female_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"vg..., outputs=Tensor(\"de...)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "graDVrcLj68c",
        "colab_type": "code",
        "outputId": "c9a7dd35-37e6-45bf-965f-daa6969b339d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "intermediate_output[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.04516411, 0.18483284, 0.16722398, ..., 0.05645609, 0.25139302,\n",
              "       0.11341704], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXva6MtWc-Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.concatenate((validation_male_list,validation_female_list,train_male_list,train_female_list), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXBxBAbUO3yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(data=data[0: ,0:],\n",
        "            index=[i for i in range(data.shape[0])],\n",
        "            columns=['f'+str(i) for i in range(data.shape[1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2mX7-Sad_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.rename(columns={'f0':'File Name'})\n",
        "df = df.set_index('File Name')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpDrOOcFdojO",
        "colab_type": "code",
        "outputId": "722eac94-44d0-4772-c588-17794c4d234d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 447 entries, SK777A.jpg to 46B.jpg\n",
            "Columns: 1024 entries, f1 to f1024\n",
            "dtypes: object(1024)\n",
            "memory usage: 3.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4nptduMRN8d",
        "colab_type": "code",
        "outputId": "796c5efc-09b9-4bad-af6f-eeea874f4c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drop_column = []\n",
        "for i in range(1,1025):\n",
        "  count = 0\n",
        "  for each in df['f' + str(i)]:\n",
        "    if float(each)<= 0.0:\n",
        "      count +=1\n",
        "  if count == len(df):\n",
        "    drop_column.append('f' + str(i))\n",
        "\n",
        "len(drop_column)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "531"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdX69H-eTjDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns = drop_column)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBuua2IyO3up",
        "colab_type": "code",
        "outputId": "67acd85d-0302-476d-b5e4-2410249e653b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f15</th>\n",
              "      <th>f18</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f36</th>\n",
              "      <th>f42</th>\n",
              "      <th>f46</th>\n",
              "      <th>f48</th>\n",
              "      <th>f49</th>\n",
              "      <th>f50</th>\n",
              "      <th>f55</th>\n",
              "      <th>f56</th>\n",
              "      <th>f61</th>\n",
              "      <th>f65</th>\n",
              "      <th>f68</th>\n",
              "      <th>f71</th>\n",
              "      <th>f73</th>\n",
              "      <th>f74</th>\n",
              "      <th>f76</th>\n",
              "      <th>f77</th>\n",
              "      <th>f80</th>\n",
              "      <th>f81</th>\n",
              "      <th>f82</th>\n",
              "      <th>...</th>\n",
              "      <th>f952</th>\n",
              "      <th>f958</th>\n",
              "      <th>f959</th>\n",
              "      <th>f960</th>\n",
              "      <th>f961</th>\n",
              "      <th>f962</th>\n",
              "      <th>f963</th>\n",
              "      <th>f964</th>\n",
              "      <th>f965</th>\n",
              "      <th>f966</th>\n",
              "      <th>f967</th>\n",
              "      <th>f968</th>\n",
              "      <th>f969</th>\n",
              "      <th>f971</th>\n",
              "      <th>f973</th>\n",
              "      <th>f976</th>\n",
              "      <th>f977</th>\n",
              "      <th>f978</th>\n",
              "      <th>f979</th>\n",
              "      <th>f985</th>\n",
              "      <th>f986</th>\n",
              "      <th>f989</th>\n",
              "      <th>f990</th>\n",
              "      <th>f991</th>\n",
              "      <th>f992</th>\n",
              "      <th>f993</th>\n",
              "      <th>f994</th>\n",
              "      <th>f995</th>\n",
              "      <th>f1001</th>\n",
              "      <th>f1006</th>\n",
              "      <th>f1010</th>\n",
              "      <th>f1012</th>\n",
              "      <th>f1013</th>\n",
              "      <th>f1015</th>\n",
              "      <th>f1016</th>\n",
              "      <th>f1019</th>\n",
              "      <th>f1020</th>\n",
              "      <th>f1022</th>\n",
              "      <th>f1023</th>\n",
              "      <th>f1024</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>File Name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SK777A.jpg</th>\n",
              "      <td>0.054964736104011536</td>\n",
              "      <td>0.22152693569660187</td>\n",
              "      <td>0.20049293339252472</td>\n",
              "      <td>0.32851511240005493</td>\n",
              "      <td>0.21281714737415314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14667686820030212</td>\n",
              "      <td>0.25303351879119873</td>\n",
              "      <td>0.20519188046455383</td>\n",
              "      <td>0.17950525879859924</td>\n",
              "      <td>0.3201408386230469</td>\n",
              "      <td>0.16100464761257172</td>\n",
              "      <td>0.437704861164093</td>\n",
              "      <td>0.22956152260303497</td>\n",
              "      <td>0.22313058376312256</td>\n",
              "      <td>0.10593216121196747</td>\n",
              "      <td>0.0904526337981224</td>\n",
              "      <td>0.15168622136116028</td>\n",
              "      <td>0.16904780268669128</td>\n",
              "      <td>0.20939067006111145</td>\n",
              "      <td>0.27041947841644287</td>\n",
              "      <td>0.36809247732162476</td>\n",
              "      <td>0.3033146858215332</td>\n",
              "      <td>0.10513006895780563</td>\n",
              "      <td>0.4112304449081421</td>\n",
              "      <td>0.3631654381752014</td>\n",
              "      <td>0.03901173919439316</td>\n",
              "      <td>0.04293293133378029</td>\n",
              "      <td>0.11261092126369476</td>\n",
              "      <td>0.14219503104686737</td>\n",
              "      <td>0.23406937718391418</td>\n",
              "      <td>0.31287750601768494</td>\n",
              "      <td>0.07245940715074539</td>\n",
              "      <td>0.34887418150901794</td>\n",
              "      <td>0.04583900421857834</td>\n",
              "      <td>0.32144853472709656</td>\n",
              "      <td>0.4295426309108734</td>\n",
              "      <td>0.04616404324769974</td>\n",
              "      <td>0.38390806317329407</td>\n",
              "      <td>0.29811912775039673</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04921803995966911</td>\n",
              "      <td>0.3273155689239502</td>\n",
              "      <td>0.2614035904407501</td>\n",
              "      <td>0.23038695752620697</td>\n",
              "      <td>0.02460755966603756</td>\n",
              "      <td>0.28379911184310913</td>\n",
              "      <td>0.4427116811275482</td>\n",
              "      <td>0.07891993969678879</td>\n",
              "      <td>0.08609309792518616</td>\n",
              "      <td>0.3025645315647125</td>\n",
              "      <td>0.26012086868286133</td>\n",
              "      <td>0.14111961424350739</td>\n",
              "      <td>0.3430349826812744</td>\n",
              "      <td>0.3789758086204529</td>\n",
              "      <td>0.2318737804889679</td>\n",
              "      <td>0.2784707844257355</td>\n",
              "      <td>0.16018834710121155</td>\n",
              "      <td>0.24888819456100464</td>\n",
              "      <td>0.5535653233528137</td>\n",
              "      <td>0.04666895046830177</td>\n",
              "      <td>0.02123383991420269</td>\n",
              "      <td>0.17220571637153625</td>\n",
              "      <td>0.3136054575443268</td>\n",
              "      <td>0.2678583562374115</td>\n",
              "      <td>0.2527478337287903</td>\n",
              "      <td>0.08554912358522415</td>\n",
              "      <td>0.28667235374450684</td>\n",
              "      <td>0.38043150305747986</td>\n",
              "      <td>0.2430526465177536</td>\n",
              "      <td>0.14139991998672485</td>\n",
              "      <td>0.4723118841648102</td>\n",
              "      <td>0.39390692114830017</td>\n",
              "      <td>0.24714842438697815</td>\n",
              "      <td>0.0793328732252121</td>\n",
              "      <td>0.37838050723075867</td>\n",
              "      <td>0.16540636122226715</td>\n",
              "      <td>0.23263463377952576</td>\n",
              "      <td>0.06758081912994385</td>\n",
              "      <td>0.3017416000366211</td>\n",
              "      <td>0.13619039952754974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74Aa.jpg</th>\n",
              "      <td>0.06117307394742966</td>\n",
              "      <td>0.2447713166475296</td>\n",
              "      <td>0.22156760096549988</td>\n",
              "      <td>0.36321821808815</td>\n",
              "      <td>0.234822615981102</td>\n",
              "      <td>0.0007261084392666817</td>\n",
              "      <td>0.16223174333572388</td>\n",
              "      <td>0.2805018723011017</td>\n",
              "      <td>0.2268441766500473</td>\n",
              "      <td>0.1980280578136444</td>\n",
              "      <td>0.3550869822502136</td>\n",
              "      <td>0.17771035432815552</td>\n",
              "      <td>0.48466986417770386</td>\n",
              "      <td>0.2544008791446686</td>\n",
              "      <td>0.24735242128372192</td>\n",
              "      <td>0.11720367521047592</td>\n",
              "      <td>0.09972742944955826</td>\n",
              "      <td>0.16760236024856567</td>\n",
              "      <td>0.1875586062669754</td>\n",
              "      <td>0.23253385722637177</td>\n",
              "      <td>0.29898691177368164</td>\n",
              "      <td>0.40726080536842346</td>\n",
              "      <td>0.3357153534889221</td>\n",
              "      <td>0.1160888820886612</td>\n",
              "      <td>0.45508795976638794</td>\n",
              "      <td>0.4019779562950134</td>\n",
              "      <td>0.04345705360174179</td>\n",
              "      <td>0.04810976982116699</td>\n",
              "      <td>0.12534472346305847</td>\n",
              "      <td>0.1577388346195221</td>\n",
              "      <td>0.2586624324321747</td>\n",
              "      <td>0.3463777005672455</td>\n",
              "      <td>0.08037487417459488</td>\n",
              "      <td>0.3863106369972229</td>\n",
              "      <td>0.05096788704395294</td>\n",
              "      <td>0.35628628730773926</td>\n",
              "      <td>0.47487673163414</td>\n",
              "      <td>0.0508415661752224</td>\n",
              "      <td>0.4245606064796448</td>\n",
              "      <td>0.3295080065727234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05504245683550835</td>\n",
              "      <td>0.36234840750694275</td>\n",
              "      <td>0.28911641240119934</td>\n",
              "      <td>0.25566017627716064</td>\n",
              "      <td>0.02772459387779236</td>\n",
              "      <td>0.3136836588382721</td>\n",
              "      <td>0.48994362354278564</td>\n",
              "      <td>0.08758337050676346</td>\n",
              "      <td>0.09578624367713928</td>\n",
              "      <td>0.3354140818119049</td>\n",
              "      <td>0.2886212468147278</td>\n",
              "      <td>0.15637604892253876</td>\n",
              "      <td>0.3794548213481903</td>\n",
              "      <td>0.4189746081829071</td>\n",
              "      <td>0.2571336627006531</td>\n",
              "      <td>0.30866560339927673</td>\n",
              "      <td>0.17818191647529602</td>\n",
              "      <td>0.27475860714912415</td>\n",
              "      <td>0.6119040250778198</td>\n",
              "      <td>0.051834724843502045</td>\n",
              "      <td>0.02349783293902874</td>\n",
              "      <td>0.19095422327518463</td>\n",
              "      <td>0.3474618196487427</td>\n",
              "      <td>0.2970238924026489</td>\n",
              "      <td>0.2798535227775574</td>\n",
              "      <td>0.09486642479896545</td>\n",
              "      <td>0.31779417395591736</td>\n",
              "      <td>0.4207954704761505</td>\n",
              "      <td>0.2695974111557007</td>\n",
              "      <td>0.1560518443584442</td>\n",
              "      <td>0.522476851940155</td>\n",
              "      <td>0.43584734201431274</td>\n",
              "      <td>0.27313971519470215</td>\n",
              "      <td>0.08736691623926163</td>\n",
              "      <td>0.41897159814834595</td>\n",
              "      <td>0.18272027373313904</td>\n",
              "      <td>0.2582162916660309</td>\n",
              "      <td>0.07462792843580246</td>\n",
              "      <td>0.3336355686187744</td>\n",
              "      <td>0.15061648190021515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK780B.jpg</th>\n",
              "      <td>0.06054367125034332</td>\n",
              "      <td>0.24241478741168976</td>\n",
              "      <td>0.21943102777004242</td>\n",
              "      <td>0.3596999943256378</td>\n",
              "      <td>0.2325916886329651</td>\n",
              "      <td>0.0006419690325856209</td>\n",
              "      <td>0.16065478324890137</td>\n",
              "      <td>0.27771711349487305</td>\n",
              "      <td>0.22464905679225922</td>\n",
              "      <td>0.19615021347999573</td>\n",
              "      <td>0.3515441417694092</td>\n",
              "      <td>0.17601671814918518</td>\n",
              "      <td>0.4799085259437561</td>\n",
              "      <td>0.2518826723098755</td>\n",
              "      <td>0.244896799325943</td>\n",
              "      <td>0.11606096476316452</td>\n",
              "      <td>0.09878714382648468</td>\n",
              "      <td>0.1659887731075287</td>\n",
              "      <td>0.18568196892738342</td>\n",
              "      <td>0.2301875799894333</td>\n",
              "      <td>0.2960907220840454</td>\n",
              "      <td>0.40328991413116455</td>\n",
              "      <td>0.33243054151535034</td>\n",
              "      <td>0.1149778664112091</td>\n",
              "      <td>0.4506416618824005</td>\n",
              "      <td>0.39804312586784363</td>\n",
              "      <td>0.04300638660788536</td>\n",
              "      <td>0.04758493974804878</td>\n",
              "      <td>0.12405376136302948</td>\n",
              "      <td>0.15616299211978912</td>\n",
              "      <td>0.2561691701412201</td>\n",
              "      <td>0.3429814279079437</td>\n",
              "      <td>0.0795724019408226</td>\n",
              "      <td>0.3825153112411499</td>\n",
              "      <td>0.050447918474674225</td>\n",
              "      <td>0.3527544140815735</td>\n",
              "      <td>0.4702807366847992</td>\n",
              "      <td>0.05036735534667969</td>\n",
              "      <td>0.420439213514328</td>\n",
              "      <td>0.32632580399513245</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05445197597146034</td>\n",
              "      <td>0.35879674553871155</td>\n",
              "      <td>0.28630685806274414</td>\n",
              "      <td>0.2530979812145233</td>\n",
              "      <td>0.02740858495235443</td>\n",
              "      <td>0.310653954744339</td>\n",
              "      <td>0.48515522480010986</td>\n",
              "      <td>0.08670506626367569</td>\n",
              "      <td>0.09480354189872742</td>\n",
              "      <td>0.3320837914943695</td>\n",
              "      <td>0.28573185205459595</td>\n",
              "      <td>0.15482933819293976</td>\n",
              "      <td>0.37576255202293396</td>\n",
              "      <td>0.41491949558258057</td>\n",
              "      <td>0.2545727789402008</td>\n",
              "      <td>0.3056044578552246</td>\n",
              "      <td>0.1763577163219452</td>\n",
              "      <td>0.27213582396507263</td>\n",
              "      <td>0.6059896349906921</td>\n",
              "      <td>0.05131101608276367</td>\n",
              "      <td>0.023268306627869606</td>\n",
              "      <td>0.1890534907579422</td>\n",
              "      <td>0.3440294563770294</td>\n",
              "      <td>0.2940670847892761</td>\n",
              "      <td>0.27710554003715515</td>\n",
              "      <td>0.09392182528972626</td>\n",
              "      <td>0.31463903188705444</td>\n",
              "      <td>0.41670334339141846</td>\n",
              "      <td>0.2669062614440918</td>\n",
              "      <td>0.1545664221048355</td>\n",
              "      <td>0.5173910856246948</td>\n",
              "      <td>0.4315953850746155</td>\n",
              "      <td>0.2705046832561493</td>\n",
              "      <td>0.08655241876840591</td>\n",
              "      <td>0.4148564338684082</td>\n",
              "      <td>0.18096497654914856</td>\n",
              "      <td>0.2556228041648865</td>\n",
              "      <td>0.07391348481178284</td>\n",
              "      <td>0.330402135848999</td>\n",
              "      <td>0.1491539478302002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK751A.jpg</th>\n",
              "      <td>0.05900680646300316</td>\n",
              "      <td>0.2366606891155243</td>\n",
              "      <td>0.2142140418291092</td>\n",
              "      <td>0.3511093258857727</td>\n",
              "      <td>0.2271442860364914</td>\n",
              "      <td>0.0004365192726254463</td>\n",
              "      <td>0.15680420398712158</td>\n",
              "      <td>0.2709173858165741</td>\n",
              "      <td>0.21928906440734863</td>\n",
              "      <td>0.1915649175643921</td>\n",
              "      <td>0.3428932726383209</td>\n",
              "      <td>0.17188125848770142</td>\n",
              "      <td>0.46828243136405945</td>\n",
              "      <td>0.24573373794555664</td>\n",
              "      <td>0.23890073597431183</td>\n",
              "      <td>0.11327072232961655</td>\n",
              "      <td>0.09649118781089783</td>\n",
              "      <td>0.16204875707626343</td>\n",
              "      <td>0.18109965324401855</td>\n",
              "      <td>0.22445853054523468</td>\n",
              "      <td>0.2890188992023468</td>\n",
              "      <td>0.39359384775161743</td>\n",
              "      <td>0.3244098424911499</td>\n",
              "      <td>0.11226503551006317</td>\n",
              "      <td>0.43978482484817505</td>\n",
              "      <td>0.38843515515327454</td>\n",
              "      <td>0.0419059582054615</td>\n",
              "      <td>0.04630342125892639</td>\n",
              "      <td>0.12090153992176056</td>\n",
              "      <td>0.152315154671669</td>\n",
              "      <td>0.25008121132850647</td>\n",
              "      <td>0.33468854427337646</td>\n",
              "      <td>0.07761294394731522</td>\n",
              "      <td>0.37324798107147217</td>\n",
              "      <td>0.04917827248573303</td>\n",
              "      <td>0.34413039684295654</td>\n",
              "      <td>0.45905837416648865</td>\n",
              "      <td>0.04920944571495056</td>\n",
              "      <td>0.41037577390670776</td>\n",
              "      <td>0.3185555338859558</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05301015079021454</td>\n",
              "      <td>0.35012444853782654</td>\n",
              "      <td>0.27944663166999817</td>\n",
              "      <td>0.24684162437915802</td>\n",
              "      <td>0.026636971160769463</td>\n",
              "      <td>0.303256094455719</td>\n",
              "      <td>0.4734630286693573</td>\n",
              "      <td>0.08456045389175415</td>\n",
              "      <td>0.09240402281284332</td>\n",
              "      <td>0.32395192980766296</td>\n",
              "      <td>0.2786766588687897</td>\n",
              "      <td>0.15105263888835907</td>\n",
              "      <td>0.3667469024658203</td>\n",
              "      <td>0.4050178825855255</td>\n",
              "      <td>0.24831975996494293</td>\n",
              "      <td>0.29812976717948914</td>\n",
              "      <td>0.17190344631671906</td>\n",
              "      <td>0.26573166251182556</td>\n",
              "      <td>0.5915480256080627</td>\n",
              "      <td>0.05003223940730095</td>\n",
              "      <td>0.02270786091685295</td>\n",
              "      <td>0.18441233038902283</td>\n",
              "      <td>0.3356483578681946</td>\n",
              "      <td>0.28684720396995544</td>\n",
              "      <td>0.27039557695388794</td>\n",
              "      <td>0.09161534905433655</td>\n",
              "      <td>0.3069348931312561</td>\n",
              "      <td>0.40671131014823914</td>\n",
              "      <td>0.26033517718315125</td>\n",
              "      <td>0.15093936026096344</td>\n",
              "      <td>0.5049728155136108</td>\n",
              "      <td>0.4212131202220917</td>\n",
              "      <td>0.264070600271225</td>\n",
              "      <td>0.08456360548734665</td>\n",
              "      <td>0.4048081934452057</td>\n",
              "      <td>0.17667895555496216</td>\n",
              "      <td>0.2492901235818863</td>\n",
              "      <td>0.0721689909696579</td>\n",
              "      <td>0.322506844997406</td>\n",
              "      <td>0.14558279514312744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK772A.jpg</th>\n",
              "      <td>0.056103043258190155</td>\n",
              "      <td>0.22578881680965424</td>\n",
              "      <td>0.20435699820518494</td>\n",
              "      <td>0.33487796783447266</td>\n",
              "      <td>0.2168518751859665</td>\n",
              "      <td>4.834122955799103e-05</td>\n",
              "      <td>0.1495288759469986</td>\n",
              "      <td>0.25806987285614014</td>\n",
              "      <td>0.20916184782981873</td>\n",
              "      <td>0.18290142714977264</td>\n",
              "      <td>0.3265482783317566</td>\n",
              "      <td>0.16406765580177307</td>\n",
              "      <td>0.4463159441947937</td>\n",
              "      <td>0.2341158539056778</td>\n",
              "      <td>0.22757169604301453</td>\n",
              "      <td>0.1079988032579422</td>\n",
              "      <td>0.09215317666530609</td>\n",
              "      <td>0.1546044647693634</td>\n",
              "      <td>0.1724417805671692</td>\n",
              "      <td>0.21363399922847748</td>\n",
              "      <td>0.2756573259830475</td>\n",
              "      <td>0.37527403235435486</td>\n",
              "      <td>0.3092553913593292</td>\n",
              "      <td>0.10713937878608704</td>\n",
              "      <td>0.4192717969417572</td>\n",
              "      <td>0.37028175592422485</td>\n",
              "      <td>0.03982679173350334</td>\n",
              "      <td>0.043882112950086594</td>\n",
              "      <td>0.1149456799030304</td>\n",
              "      <td>0.14504501223564148</td>\n",
              "      <td>0.23857854306697845</td>\n",
              "      <td>0.3190198242664337</td>\n",
              "      <td>0.07391072064638138</td>\n",
              "      <td>0.35573819279670715</td>\n",
              "      <td>0.046779390424489975</td>\n",
              "      <td>0.3278360664844513</td>\n",
              "      <td>0.43785467743873596</td>\n",
              "      <td>0.04702167212963104</td>\n",
              "      <td>0.3913617432117462</td>\n",
              "      <td>0.30387431383132935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050285954028367996</td>\n",
              "      <td>0.33373886346817017</td>\n",
              "      <td>0.2664847671985626</td>\n",
              "      <td>0.23502083122730255</td>\n",
              "      <td>0.025179071351885796</td>\n",
              "      <td>0.28927847743034363</td>\n",
              "      <td>0.4513716995716095</td>\n",
              "      <td>0.08050838857889175</td>\n",
              "      <td>0.08787034451961517</td>\n",
              "      <td>0.30858752131462097</td>\n",
              "      <td>0.2653464376926422</td>\n",
              "      <td>0.14391690492630005</td>\n",
              "      <td>0.349712610244751</td>\n",
              "      <td>0.3863096237182617</td>\n",
              "      <td>0.2365051954984665</td>\n",
              "      <td>0.2840070426464081</td>\n",
              "      <td>0.1634874939918518</td>\n",
              "      <td>0.2536315619945526</td>\n",
              "      <td>0.564261794090271</td>\n",
              "      <td>0.047616101801395416</td>\n",
              "      <td>0.021648945286870003</td>\n",
              "      <td>0.17564328014850616</td>\n",
              "      <td>0.319813072681427</td>\n",
              "      <td>0.27320587635040283</td>\n",
              "      <td>0.25771769881248474</td>\n",
              "      <td>0.08725745975971222</td>\n",
              "      <td>0.29237857460975647</td>\n",
              "      <td>0.38783228397369385</td>\n",
              "      <td>0.24791964888572693</td>\n",
              "      <td>0.14408636093139648</td>\n",
              "      <td>0.4815097153186798</td>\n",
              "      <td>0.4015967547893524</td>\n",
              "      <td>0.2519139349460602</td>\n",
              "      <td>0.08080592006444931</td>\n",
              "      <td>0.38582292199134827</td>\n",
              "      <td>0.16858088970184326</td>\n",
              "      <td>0.237325057387352</td>\n",
              "      <td>0.06887291371822357</td>\n",
              "      <td>0.3075893521308899</td>\n",
              "      <td>0.13883543014526367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 493 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              f1  ...                f1024\n",
              "File Name                         ...                     \n",
              "SK777A.jpg  0.054964736104011536  ...  0.13619039952754974\n",
              "74Aa.jpg     0.06117307394742966  ...  0.15061648190021515\n",
              "SK780B.jpg   0.06054367125034332  ...   0.1491539478302002\n",
              "SK751A.jpg   0.05900680646300316  ...  0.14558279514312744\n",
              "SK772A.jpg  0.056103043258190155  ...  0.13883543014526367\n",
              "\n",
              "[5 rows x 493 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEGnDV4U-P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_name = []\n",
        "for i in range(1,494):\n",
        "  column_name.append('f' + str(i))\n",
        "\n",
        "df.columns = column_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtvJHSApVdBL",
        "colab_type": "code",
        "outputId": "e2f7bb1d-1c6e-4006-c65e-37461d1bee00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f35</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>f39</th>\n",
              "      <th>f40</th>\n",
              "      <th>...</th>\n",
              "      <th>f454</th>\n",
              "      <th>f455</th>\n",
              "      <th>f456</th>\n",
              "      <th>f457</th>\n",
              "      <th>f458</th>\n",
              "      <th>f459</th>\n",
              "      <th>f460</th>\n",
              "      <th>f461</th>\n",
              "      <th>f462</th>\n",
              "      <th>f463</th>\n",
              "      <th>f464</th>\n",
              "      <th>f465</th>\n",
              "      <th>f466</th>\n",
              "      <th>f467</th>\n",
              "      <th>f468</th>\n",
              "      <th>f469</th>\n",
              "      <th>f470</th>\n",
              "      <th>f471</th>\n",
              "      <th>f472</th>\n",
              "      <th>f473</th>\n",
              "      <th>f474</th>\n",
              "      <th>f475</th>\n",
              "      <th>f476</th>\n",
              "      <th>f477</th>\n",
              "      <th>f478</th>\n",
              "      <th>f479</th>\n",
              "      <th>f480</th>\n",
              "      <th>f481</th>\n",
              "      <th>f482</th>\n",
              "      <th>f483</th>\n",
              "      <th>f484</th>\n",
              "      <th>f485</th>\n",
              "      <th>f486</th>\n",
              "      <th>f487</th>\n",
              "      <th>f488</th>\n",
              "      <th>f489</th>\n",
              "      <th>f490</th>\n",
              "      <th>f491</th>\n",
              "      <th>f492</th>\n",
              "      <th>f493</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>File Name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SK777A.jpg</th>\n",
              "      <td>0.054964736104011536</td>\n",
              "      <td>0.22152693569660187</td>\n",
              "      <td>0.20049293339252472</td>\n",
              "      <td>0.32851511240005493</td>\n",
              "      <td>0.21281714737415314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14667686820030212</td>\n",
              "      <td>0.25303351879119873</td>\n",
              "      <td>0.20519188046455383</td>\n",
              "      <td>0.17950525879859924</td>\n",
              "      <td>0.3201408386230469</td>\n",
              "      <td>0.16100464761257172</td>\n",
              "      <td>0.437704861164093</td>\n",
              "      <td>0.22956152260303497</td>\n",
              "      <td>0.22313058376312256</td>\n",
              "      <td>0.10593216121196747</td>\n",
              "      <td>0.0904526337981224</td>\n",
              "      <td>0.15168622136116028</td>\n",
              "      <td>0.16904780268669128</td>\n",
              "      <td>0.20939067006111145</td>\n",
              "      <td>0.27041947841644287</td>\n",
              "      <td>0.36809247732162476</td>\n",
              "      <td>0.3033146858215332</td>\n",
              "      <td>0.10513006895780563</td>\n",
              "      <td>0.4112304449081421</td>\n",
              "      <td>0.3631654381752014</td>\n",
              "      <td>0.03901173919439316</td>\n",
              "      <td>0.04293293133378029</td>\n",
              "      <td>0.11261092126369476</td>\n",
              "      <td>0.14219503104686737</td>\n",
              "      <td>0.23406937718391418</td>\n",
              "      <td>0.31287750601768494</td>\n",
              "      <td>0.07245940715074539</td>\n",
              "      <td>0.34887418150901794</td>\n",
              "      <td>0.04583900421857834</td>\n",
              "      <td>0.32144853472709656</td>\n",
              "      <td>0.4295426309108734</td>\n",
              "      <td>0.04616404324769974</td>\n",
              "      <td>0.38390806317329407</td>\n",
              "      <td>0.29811912775039673</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04921803995966911</td>\n",
              "      <td>0.3273155689239502</td>\n",
              "      <td>0.2614035904407501</td>\n",
              "      <td>0.23038695752620697</td>\n",
              "      <td>0.02460755966603756</td>\n",
              "      <td>0.28379911184310913</td>\n",
              "      <td>0.4427116811275482</td>\n",
              "      <td>0.07891993969678879</td>\n",
              "      <td>0.08609309792518616</td>\n",
              "      <td>0.3025645315647125</td>\n",
              "      <td>0.26012086868286133</td>\n",
              "      <td>0.14111961424350739</td>\n",
              "      <td>0.3430349826812744</td>\n",
              "      <td>0.3789758086204529</td>\n",
              "      <td>0.2318737804889679</td>\n",
              "      <td>0.2784707844257355</td>\n",
              "      <td>0.16018834710121155</td>\n",
              "      <td>0.24888819456100464</td>\n",
              "      <td>0.5535653233528137</td>\n",
              "      <td>0.04666895046830177</td>\n",
              "      <td>0.02123383991420269</td>\n",
              "      <td>0.17220571637153625</td>\n",
              "      <td>0.3136054575443268</td>\n",
              "      <td>0.2678583562374115</td>\n",
              "      <td>0.2527478337287903</td>\n",
              "      <td>0.08554912358522415</td>\n",
              "      <td>0.28667235374450684</td>\n",
              "      <td>0.38043150305747986</td>\n",
              "      <td>0.2430526465177536</td>\n",
              "      <td>0.14139991998672485</td>\n",
              "      <td>0.4723118841648102</td>\n",
              "      <td>0.39390692114830017</td>\n",
              "      <td>0.24714842438697815</td>\n",
              "      <td>0.0793328732252121</td>\n",
              "      <td>0.37838050723075867</td>\n",
              "      <td>0.16540636122226715</td>\n",
              "      <td>0.23263463377952576</td>\n",
              "      <td>0.06758081912994385</td>\n",
              "      <td>0.3017416000366211</td>\n",
              "      <td>0.13619039952754974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74Aa.jpg</th>\n",
              "      <td>0.06117307394742966</td>\n",
              "      <td>0.2447713166475296</td>\n",
              "      <td>0.22156760096549988</td>\n",
              "      <td>0.36321821808815</td>\n",
              "      <td>0.234822615981102</td>\n",
              "      <td>0.0007261084392666817</td>\n",
              "      <td>0.16223174333572388</td>\n",
              "      <td>0.2805018723011017</td>\n",
              "      <td>0.2268441766500473</td>\n",
              "      <td>0.1980280578136444</td>\n",
              "      <td>0.3550869822502136</td>\n",
              "      <td>0.17771035432815552</td>\n",
              "      <td>0.48466986417770386</td>\n",
              "      <td>0.2544008791446686</td>\n",
              "      <td>0.24735242128372192</td>\n",
              "      <td>0.11720367521047592</td>\n",
              "      <td>0.09972742944955826</td>\n",
              "      <td>0.16760236024856567</td>\n",
              "      <td>0.1875586062669754</td>\n",
              "      <td>0.23253385722637177</td>\n",
              "      <td>0.29898691177368164</td>\n",
              "      <td>0.40726080536842346</td>\n",
              "      <td>0.3357153534889221</td>\n",
              "      <td>0.1160888820886612</td>\n",
              "      <td>0.45508795976638794</td>\n",
              "      <td>0.4019779562950134</td>\n",
              "      <td>0.04345705360174179</td>\n",
              "      <td>0.04810976982116699</td>\n",
              "      <td>0.12534472346305847</td>\n",
              "      <td>0.1577388346195221</td>\n",
              "      <td>0.2586624324321747</td>\n",
              "      <td>0.3463777005672455</td>\n",
              "      <td>0.08037487417459488</td>\n",
              "      <td>0.3863106369972229</td>\n",
              "      <td>0.05096788704395294</td>\n",
              "      <td>0.35628628730773926</td>\n",
              "      <td>0.47487673163414</td>\n",
              "      <td>0.0508415661752224</td>\n",
              "      <td>0.4245606064796448</td>\n",
              "      <td>0.3295080065727234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05504245683550835</td>\n",
              "      <td>0.36234840750694275</td>\n",
              "      <td>0.28911641240119934</td>\n",
              "      <td>0.25566017627716064</td>\n",
              "      <td>0.02772459387779236</td>\n",
              "      <td>0.3136836588382721</td>\n",
              "      <td>0.48994362354278564</td>\n",
              "      <td>0.08758337050676346</td>\n",
              "      <td>0.09578624367713928</td>\n",
              "      <td>0.3354140818119049</td>\n",
              "      <td>0.2886212468147278</td>\n",
              "      <td>0.15637604892253876</td>\n",
              "      <td>0.3794548213481903</td>\n",
              "      <td>0.4189746081829071</td>\n",
              "      <td>0.2571336627006531</td>\n",
              "      <td>0.30866560339927673</td>\n",
              "      <td>0.17818191647529602</td>\n",
              "      <td>0.27475860714912415</td>\n",
              "      <td>0.6119040250778198</td>\n",
              "      <td>0.051834724843502045</td>\n",
              "      <td>0.02349783293902874</td>\n",
              "      <td>0.19095422327518463</td>\n",
              "      <td>0.3474618196487427</td>\n",
              "      <td>0.2970238924026489</td>\n",
              "      <td>0.2798535227775574</td>\n",
              "      <td>0.09486642479896545</td>\n",
              "      <td>0.31779417395591736</td>\n",
              "      <td>0.4207954704761505</td>\n",
              "      <td>0.2695974111557007</td>\n",
              "      <td>0.1560518443584442</td>\n",
              "      <td>0.522476851940155</td>\n",
              "      <td>0.43584734201431274</td>\n",
              "      <td>0.27313971519470215</td>\n",
              "      <td>0.08736691623926163</td>\n",
              "      <td>0.41897159814834595</td>\n",
              "      <td>0.18272027373313904</td>\n",
              "      <td>0.2582162916660309</td>\n",
              "      <td>0.07462792843580246</td>\n",
              "      <td>0.3336355686187744</td>\n",
              "      <td>0.15061648190021515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK780B.jpg</th>\n",
              "      <td>0.06054367125034332</td>\n",
              "      <td>0.24241478741168976</td>\n",
              "      <td>0.21943102777004242</td>\n",
              "      <td>0.3596999943256378</td>\n",
              "      <td>0.2325916886329651</td>\n",
              "      <td>0.0006419690325856209</td>\n",
              "      <td>0.16065478324890137</td>\n",
              "      <td>0.27771711349487305</td>\n",
              "      <td>0.22464905679225922</td>\n",
              "      <td>0.19615021347999573</td>\n",
              "      <td>0.3515441417694092</td>\n",
              "      <td>0.17601671814918518</td>\n",
              "      <td>0.4799085259437561</td>\n",
              "      <td>0.2518826723098755</td>\n",
              "      <td>0.244896799325943</td>\n",
              "      <td>0.11606096476316452</td>\n",
              "      <td>0.09878714382648468</td>\n",
              "      <td>0.1659887731075287</td>\n",
              "      <td>0.18568196892738342</td>\n",
              "      <td>0.2301875799894333</td>\n",
              "      <td>0.2960907220840454</td>\n",
              "      <td>0.40328991413116455</td>\n",
              "      <td>0.33243054151535034</td>\n",
              "      <td>0.1149778664112091</td>\n",
              "      <td>0.4506416618824005</td>\n",
              "      <td>0.39804312586784363</td>\n",
              "      <td>0.04300638660788536</td>\n",
              "      <td>0.04758493974804878</td>\n",
              "      <td>0.12405376136302948</td>\n",
              "      <td>0.15616299211978912</td>\n",
              "      <td>0.2561691701412201</td>\n",
              "      <td>0.3429814279079437</td>\n",
              "      <td>0.0795724019408226</td>\n",
              "      <td>0.3825153112411499</td>\n",
              "      <td>0.050447918474674225</td>\n",
              "      <td>0.3527544140815735</td>\n",
              "      <td>0.4702807366847992</td>\n",
              "      <td>0.05036735534667969</td>\n",
              "      <td>0.420439213514328</td>\n",
              "      <td>0.32632580399513245</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05445197597146034</td>\n",
              "      <td>0.35879674553871155</td>\n",
              "      <td>0.28630685806274414</td>\n",
              "      <td>0.2530979812145233</td>\n",
              "      <td>0.02740858495235443</td>\n",
              "      <td>0.310653954744339</td>\n",
              "      <td>0.48515522480010986</td>\n",
              "      <td>0.08670506626367569</td>\n",
              "      <td>0.09480354189872742</td>\n",
              "      <td>0.3320837914943695</td>\n",
              "      <td>0.28573185205459595</td>\n",
              "      <td>0.15482933819293976</td>\n",
              "      <td>0.37576255202293396</td>\n",
              "      <td>0.41491949558258057</td>\n",
              "      <td>0.2545727789402008</td>\n",
              "      <td>0.3056044578552246</td>\n",
              "      <td>0.1763577163219452</td>\n",
              "      <td>0.27213582396507263</td>\n",
              "      <td>0.6059896349906921</td>\n",
              "      <td>0.05131101608276367</td>\n",
              "      <td>0.023268306627869606</td>\n",
              "      <td>0.1890534907579422</td>\n",
              "      <td>0.3440294563770294</td>\n",
              "      <td>0.2940670847892761</td>\n",
              "      <td>0.27710554003715515</td>\n",
              "      <td>0.09392182528972626</td>\n",
              "      <td>0.31463903188705444</td>\n",
              "      <td>0.41670334339141846</td>\n",
              "      <td>0.2669062614440918</td>\n",
              "      <td>0.1545664221048355</td>\n",
              "      <td>0.5173910856246948</td>\n",
              "      <td>0.4315953850746155</td>\n",
              "      <td>0.2705046832561493</td>\n",
              "      <td>0.08655241876840591</td>\n",
              "      <td>0.4148564338684082</td>\n",
              "      <td>0.18096497654914856</td>\n",
              "      <td>0.2556228041648865</td>\n",
              "      <td>0.07391348481178284</td>\n",
              "      <td>0.330402135848999</td>\n",
              "      <td>0.1491539478302002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK751A.jpg</th>\n",
              "      <td>0.05900680646300316</td>\n",
              "      <td>0.2366606891155243</td>\n",
              "      <td>0.2142140418291092</td>\n",
              "      <td>0.3511093258857727</td>\n",
              "      <td>0.2271442860364914</td>\n",
              "      <td>0.0004365192726254463</td>\n",
              "      <td>0.15680420398712158</td>\n",
              "      <td>0.2709173858165741</td>\n",
              "      <td>0.21928906440734863</td>\n",
              "      <td>0.1915649175643921</td>\n",
              "      <td>0.3428932726383209</td>\n",
              "      <td>0.17188125848770142</td>\n",
              "      <td>0.46828243136405945</td>\n",
              "      <td>0.24573373794555664</td>\n",
              "      <td>0.23890073597431183</td>\n",
              "      <td>0.11327072232961655</td>\n",
              "      <td>0.09649118781089783</td>\n",
              "      <td>0.16204875707626343</td>\n",
              "      <td>0.18109965324401855</td>\n",
              "      <td>0.22445853054523468</td>\n",
              "      <td>0.2890188992023468</td>\n",
              "      <td>0.39359384775161743</td>\n",
              "      <td>0.3244098424911499</td>\n",
              "      <td>0.11226503551006317</td>\n",
              "      <td>0.43978482484817505</td>\n",
              "      <td>0.38843515515327454</td>\n",
              "      <td>0.0419059582054615</td>\n",
              "      <td>0.04630342125892639</td>\n",
              "      <td>0.12090153992176056</td>\n",
              "      <td>0.152315154671669</td>\n",
              "      <td>0.25008121132850647</td>\n",
              "      <td>0.33468854427337646</td>\n",
              "      <td>0.07761294394731522</td>\n",
              "      <td>0.37324798107147217</td>\n",
              "      <td>0.04917827248573303</td>\n",
              "      <td>0.34413039684295654</td>\n",
              "      <td>0.45905837416648865</td>\n",
              "      <td>0.04920944571495056</td>\n",
              "      <td>0.41037577390670776</td>\n",
              "      <td>0.3185555338859558</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05301015079021454</td>\n",
              "      <td>0.35012444853782654</td>\n",
              "      <td>0.27944663166999817</td>\n",
              "      <td>0.24684162437915802</td>\n",
              "      <td>0.026636971160769463</td>\n",
              "      <td>0.303256094455719</td>\n",
              "      <td>0.4734630286693573</td>\n",
              "      <td>0.08456045389175415</td>\n",
              "      <td>0.09240402281284332</td>\n",
              "      <td>0.32395192980766296</td>\n",
              "      <td>0.2786766588687897</td>\n",
              "      <td>0.15105263888835907</td>\n",
              "      <td>0.3667469024658203</td>\n",
              "      <td>0.4050178825855255</td>\n",
              "      <td>0.24831975996494293</td>\n",
              "      <td>0.29812976717948914</td>\n",
              "      <td>0.17190344631671906</td>\n",
              "      <td>0.26573166251182556</td>\n",
              "      <td>0.5915480256080627</td>\n",
              "      <td>0.05003223940730095</td>\n",
              "      <td>0.02270786091685295</td>\n",
              "      <td>0.18441233038902283</td>\n",
              "      <td>0.3356483578681946</td>\n",
              "      <td>0.28684720396995544</td>\n",
              "      <td>0.27039557695388794</td>\n",
              "      <td>0.09161534905433655</td>\n",
              "      <td>0.3069348931312561</td>\n",
              "      <td>0.40671131014823914</td>\n",
              "      <td>0.26033517718315125</td>\n",
              "      <td>0.15093936026096344</td>\n",
              "      <td>0.5049728155136108</td>\n",
              "      <td>0.4212131202220917</td>\n",
              "      <td>0.264070600271225</td>\n",
              "      <td>0.08456360548734665</td>\n",
              "      <td>0.4048081934452057</td>\n",
              "      <td>0.17667895555496216</td>\n",
              "      <td>0.2492901235818863</td>\n",
              "      <td>0.0721689909696579</td>\n",
              "      <td>0.322506844997406</td>\n",
              "      <td>0.14558279514312744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SK772A.jpg</th>\n",
              "      <td>0.056103043258190155</td>\n",
              "      <td>0.22578881680965424</td>\n",
              "      <td>0.20435699820518494</td>\n",
              "      <td>0.33487796783447266</td>\n",
              "      <td>0.2168518751859665</td>\n",
              "      <td>4.834122955799103e-05</td>\n",
              "      <td>0.1495288759469986</td>\n",
              "      <td>0.25806987285614014</td>\n",
              "      <td>0.20916184782981873</td>\n",
              "      <td>0.18290142714977264</td>\n",
              "      <td>0.3265482783317566</td>\n",
              "      <td>0.16406765580177307</td>\n",
              "      <td>0.4463159441947937</td>\n",
              "      <td>0.2341158539056778</td>\n",
              "      <td>0.22757169604301453</td>\n",
              "      <td>0.1079988032579422</td>\n",
              "      <td>0.09215317666530609</td>\n",
              "      <td>0.1546044647693634</td>\n",
              "      <td>0.1724417805671692</td>\n",
              "      <td>0.21363399922847748</td>\n",
              "      <td>0.2756573259830475</td>\n",
              "      <td>0.37527403235435486</td>\n",
              "      <td>0.3092553913593292</td>\n",
              "      <td>0.10713937878608704</td>\n",
              "      <td>0.4192717969417572</td>\n",
              "      <td>0.37028175592422485</td>\n",
              "      <td>0.03982679173350334</td>\n",
              "      <td>0.043882112950086594</td>\n",
              "      <td>0.1149456799030304</td>\n",
              "      <td>0.14504501223564148</td>\n",
              "      <td>0.23857854306697845</td>\n",
              "      <td>0.3190198242664337</td>\n",
              "      <td>0.07391072064638138</td>\n",
              "      <td>0.35573819279670715</td>\n",
              "      <td>0.046779390424489975</td>\n",
              "      <td>0.3278360664844513</td>\n",
              "      <td>0.43785467743873596</td>\n",
              "      <td>0.04702167212963104</td>\n",
              "      <td>0.3913617432117462</td>\n",
              "      <td>0.30387431383132935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050285954028367996</td>\n",
              "      <td>0.33373886346817017</td>\n",
              "      <td>0.2664847671985626</td>\n",
              "      <td>0.23502083122730255</td>\n",
              "      <td>0.025179071351885796</td>\n",
              "      <td>0.28927847743034363</td>\n",
              "      <td>0.4513716995716095</td>\n",
              "      <td>0.08050838857889175</td>\n",
              "      <td>0.08787034451961517</td>\n",
              "      <td>0.30858752131462097</td>\n",
              "      <td>0.2653464376926422</td>\n",
              "      <td>0.14391690492630005</td>\n",
              "      <td>0.349712610244751</td>\n",
              "      <td>0.3863096237182617</td>\n",
              "      <td>0.2365051954984665</td>\n",
              "      <td>0.2840070426464081</td>\n",
              "      <td>0.1634874939918518</td>\n",
              "      <td>0.2536315619945526</td>\n",
              "      <td>0.564261794090271</td>\n",
              "      <td>0.047616101801395416</td>\n",
              "      <td>0.021648945286870003</td>\n",
              "      <td>0.17564328014850616</td>\n",
              "      <td>0.319813072681427</td>\n",
              "      <td>0.27320587635040283</td>\n",
              "      <td>0.25771769881248474</td>\n",
              "      <td>0.08725745975971222</td>\n",
              "      <td>0.29237857460975647</td>\n",
              "      <td>0.38783228397369385</td>\n",
              "      <td>0.24791964888572693</td>\n",
              "      <td>0.14408636093139648</td>\n",
              "      <td>0.4815097153186798</td>\n",
              "      <td>0.4015967547893524</td>\n",
              "      <td>0.2519139349460602</td>\n",
              "      <td>0.08080592006444931</td>\n",
              "      <td>0.38582292199134827</td>\n",
              "      <td>0.16858088970184326</td>\n",
              "      <td>0.237325057387352</td>\n",
              "      <td>0.06887291371822357</td>\n",
              "      <td>0.3075893521308899</td>\n",
              "      <td>0.13883543014526367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 493 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              f1  ...                 f493\n",
              "File Name                         ...                     \n",
              "SK777A.jpg  0.054964736104011536  ...  0.13619039952754974\n",
              "74Aa.jpg     0.06117307394742966  ...  0.15061648190021515\n",
              "SK780B.jpg   0.06054367125034332  ...   0.1491539478302002\n",
              "SK751A.jpg   0.05900680646300316  ...  0.14558279514312744\n",
              "SK772A.jpg  0.056103043258190155  ...  0.13883543014526367\n",
              "\n",
              "[5 rows x 493 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyRJWzqbO3rU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " df.to_excel('output.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}